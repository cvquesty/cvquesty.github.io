<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Questy.org]]></title>
  <link href="http://questy.org/atom.xml" rel="self"/>
  <link href="http://questy.org/"/>
  <updated>2017-04-23T19:20:57-04:00</updated>
  <id>http://questy.org/</id>
  <author>
    <name><![CDATA[Jerald Sheets]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scaling Puppet Enterprise - Part III - Additional Compilers - Part 2]]></title>
    <link href="http://questy.org/blog/2017/04/23/scaling-puppet-enterprise-part-iii-additional-compilers-part-2/"/>
    <updated>2017-04-23T18:19:42-04:00</updated>
    <id>http://questy.org/blog/2017/04/23/scaling-puppet-enterprise-part-iii-additional-compilers-part-2</id>
    <content type="html"><![CDATA[<p><strong><em>As in the previous installment, you need to have already completed a few steps before arriving at this post.  You should have already completed a &ldquo;split installation&rdquo; (Documented <a href="http://questy.org/blog/2017/04/18/scaling-puppet-enterprise-part-ii-installation/">here</a>). Also, your load balancer needs to be configured and running. The procedure for this portion can be found <a href="http://questy.org/blog/2017/04/21/scaling-puppet-enterprise-part-iii-additional-compilers-part-1/">here</a>.  If you&rsquo;ve completed all these portions, you are now ready to configure and install the compilers themselves. If this is you, read on!</em></strong></p>

<hr />

<p>Once your Load Balancer and split install are in place and functioning, we need to add more compilers to the serving infrastructure. For the purposes of this tutorial, we will install two additional catalog compilers, register them with the currently existing master. Then, we will direct them to look to the &ldquo;MoM&rdquo; or the &ldquo;Master of Masters&rdquo; as the CA certificate authority.  Further, we will install two agent nodes and connect them to the infrastructure.</p>

<p>You will need to install two compiler nodes and two agent nodes according to the following specifications.</p>

<p><img src="http://cvquesty.github.io/images/compiler_and_agent_specs.png" alt="Compilers and Agents" /></p>

<p>Once these four nodes are in place, we can connect the compilers to the Master of Masters (MoM) and then the agents to the &ldquo;master&rdquo; as they see it.  Remember, that for our purposes, these nodes are named:</p>

<ul>
<li>compile1.example.com</li>
<li>compile2.example.com</li>
<li>compile3.example.com</li>
<li>compile4.example.com</li>
</ul>


<h3>Installing the Compilers</h3>

<p>SSH to the first compiler master (compile1.example.com for this post&rsquo;s purposes) and install the Puppet agent as follows:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -k https://master.example.com:8140/packages/current/install.bash | bash -s main:dns_alt_names=compile1.example.com,compile.example.com,compile1,compile</span></code></pre></td></tr></table></div></figure>


<p>What this does is simple. When this compiler goes behind the load balancer (compile.example.com), traffic may get directed to this node. When the request is made, the agent node will be asking for &ldquo;compile.example.com&rdquo; but this node&rsquo;s name is &ldquo;compile1.example.com&rdquo;. The additional options at the end of the curl line are to tell the agent that when it installs, it should be aware of both names, and when speaking to the MoM the first time to request its cert, to represent all the comma delimited names listed at the end of the above command.</p>

<p>Next, SSH to your master node and accept the agent cert request as follows to allow for these names on the MoM you just set up in the previous step.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh master.example.com
</span><span class='line'>puppet cert --allow-dns-alt-names sign compile1.example.com
</span></code></pre></td></tr></table></div></figure>


<h4>NOTE THAT YOU CANNOT ACCEPT THIS CERT FROM THE CONSOLE. ALT_DNS IS NOT SUPPORTED FROM THE GUI</h4>

<p>Finally, run the puppet agent on the first compiler <em>(compile1.example.com)</em> to configure the node:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>puppet agent -t</span></code></pre></td></tr></table></div></figure>


<p>Once the agent run is complete, you need to classify the catlog compiler in the console to make it ready for service.</p>

<h3>Classify the Compiler</h3>

<p>In the Puppet Enterprise Console:</p>

<p>Choose Nodes | Classification | PE Master</p>

<p>Add compile1.example.com and pin the node to the classification group and commit the change.</p>

<p><strong><strong><em>BE SURE TO COMPLETE THE NEXT STEPS IN ORDER AS FOLLOWS OR YOU WILL HAVE A BAD TIME</em></strong></strong></p>

<p>First: SSH to compile1.example.com and run <strong><em>puppet agent -t</em></strong></p>

<p>Second: SSH to puppetdb.example.com and run <strong><em>puppet agent -t</em></strong></p>

<p>Third: SSH to console.example.com and run <strong><em>puppet agent -t</em></strong></p>

<p>Fourth: SSH to master.example.com and run <strong><em>puppet agent -t</em></strong></p>

<p><strong><strong><em>BE SURE TO ALLOW EACH RUN TO COMPLETE <em>FULLY</em> BEFORE MOVING ON TO THE NEXT ONE</em></strong></strong></p>

<h3>For All Subsequent Compile Node Installations</h3>

<p>Follow the above instructions completed for compile1.example.com for all subsequent compiler installations. This means that if you add compilers six months or a year from now, go back to the previous procedure and duplicate it with the new node name precisely as you did above.  To recap:</p>

<ol>
<li>Install the agent as above with the alt_dns switches</li>
<li>Accept the cert on the master with the alt_dns switches</li>
<li>Classify the compiler in the console</li>
<li>Run the Puppet agent in the above specified order, allowing each one to complete fully before moving on.</li>
</ol>


<h3>Configure Future Agent Installations to Point to the Load Balancer by Default</h3>

<p>In the Puppet Enterprise Console, you must configure the system to point all future agent installations to the load balancer by default so you do not have to continue to make modifications and customizations after each agent install. To do so, perform the following steps:</p>

<ol>
<li>In the Puppet Enterprise Console, choose: Nodes | Classification | PE Master</li>
<li>Select the &ldquo;Classes&rdquo; tab.</li>
<li>Choose the &ldquo;pe_repo&rdquo; class.</li>
<li>Under the parameters drop-down, choose &ldquo;master&rdquo; and set the text box to the name of your load balancer or VIP <em>(in our case, &ldquo;compile.example.com&rdquo;)</em></li>
<li>Commit the changes.</li>
</ol>


<h3>Point the New Compilers at the Master (MoM) for CA Authority</h3>

<h4>Create a new classification group called &ldquo;PE CA pe_repo Override&rdquo;</h4>

<ul>
<li>Go to Nodes | Classification in the Puppet Enterprise Console</li>
<li>Create a New Group</li>
<li>Name the new group &ldquo;PE CA pe_repo Override&rdquo;</li>
<li>From the &ldquo;Parent Name&rdquo; drop-down, choose the &ldquo;PE Master&rdquo; group.</li>
<li><p>Click &ldquo;Add Group&rdquo;.</p></li>
<li><p>Select your new group and pin master.example.com to the new group and click &ldquo;Commit One Change&rdquo;</p></li>
<li>Select the &ldquo;Classes&rdquo; tab.</li>
<li>Add &ldquo;pe_repo&rdquo; class.</li>
<li>From the parameter drop-down, select &ldquo;Master&rdquo;.</li>
<li>Enter the name of the MoM in the text box. <em>(in this example, master.example.com)</em></li>
<li>Click &ldquo;Add Parameter&rdquo; and then &ldquo;Commit 2 Changes&rdquo;.</li>
</ul>


<h4>Test New Agents</h4>

<p>The two agents you created at the beginning of the article are now able to be tested with this new group of compilers.</p>

<ol>
<li>Make sure agent1 and agent2.example.com have been installed according to the system requirements covered in this series.</li>
<li>Install the Puppet agent on each of these nodes, but this time instead of pointing at the MoM, point to your Load balancer vip like so:</li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -k https://compile.example.com:8140/packages/current/install.bash | bash</span></code></pre></td></tr></table></div></figure>


<p>In the PE Console, accept the new certificate request for Agent1.  SSH to agent1.example.com and run <em><strong>puppet agent -t</strong></em>.  Finally, repeat this process for agent2.example.com.</p>

<p>If you have completed all the above steps properly, the agents will reach out to the compile.example.com VIP and be ferried off to one of the catalog compilers. Regardless of which one, since we accepted all the alternate DNS names when creating the connection between them and the MoM, they will respond for compile.example.com, and deliver back to the agent the required information, catalog, etc. as Puppet would do under normal circumstances.</p>

<h3>Conclusion</h3>

<p>As you can see, we needed the Load Balancer in place to install the catalog compilers. We also needed all the DNS alt-naming to be in place so the load balancer could send traffic to either catalog compiler as needed, and still have it answer for the VIP name. Finally, we needed to refer requests to their appropriate destinations and also classify the new compilers as such with the MoM, and set up appropriate referral of certificate requests from the compilers back to the CA Master, which is the MoM.</p>

<p>The serving infrstructure is almost done, all we have left to do is to scale MCollective with an ActiveMQ Hub &amp; Spoke, and remove that responsibility from the MoM.  We will also install a GitLab server to hold our Control Repo and associated Roles &amp; Profiles, and we will configure the Code Manager.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scaling Puppet Enterprise - Part III - Additional Compilers - Part 1]]></title>
    <link href="http://questy.org/blog/2017/04/21/scaling-puppet-enterprise-part-iii-additional-compilers-part-1/"/>
    <updated>2017-04-21T09:32:47-04:00</updated>
    <id>http://questy.org/blog/2017/04/21/scaling-puppet-enterprise-part-iii-additional-compilers-part-1</id>
    <content type="html"><![CDATA[<p><strong><em>You should have completed a split install before beginning this section. You can find the Split Installation documentation at Puppet&rsquo;s Website, or the first installment of this tutorial <a href="http://questy.org/blog/2017/04/18/scaling-puppet-enterprise-part-ii-installation/">here</a>.  If you try and begin here, you might find yourself lost.</em></strong></p>

<p><strong><em>Note also that the &ldquo;Additional Compilers&rdquo; docs comes in two parts&ndash;One to install the Load Balancer and one to install the compilers.</em></strong></p>

<h3>First, Some Philosophy</h3>

<p>The Puppet Enterprise documentation circa PE 2015.3.2 had some &ldquo;issues&rdquo;. Let me actually preface that, though. Puppet Labs' documentation is by far some of the most voluminous and in many respects most complete vendor documentation out there. I don&rsquo;t mean to disparage their work AT ALL. When it comes to the fact they even have documentation at this level, they&rsquo;re the &ldquo;bees knees&rdquo;.</p>

<p>However, I&rsquo;ve always written documentation to fit the &ldquo;grandma rule&rdquo;. My grandmother was a little 4 foot nothing Cajun woman with English as her second language.  She never used the first computer, still had a rotary phone when she passed away, and remained suspicious of anything technical.  She <em>was</em>, however, a voracious reader, keenly intelligent, and understood considerably more than you&rsquo;d expect on first glance. She also was a stickler for puncutation, grammar, and the like. In short, if my grandma couldn&rsquo;t read the documentation and follow a step-by-step process to install Puppet successfully, then its just either too complex, poorly formatted or unclear and needs to be simplified.</p>

<p>This causes a problem, of course. There are technologists out there that would become annoyed at repetition, verbosity around &ldquo;understood&rdquo; things, and spelling out each and every step along the way&hellip; even painfully. However, I feel it is the only <em>proper</em> way to document something. My rules are simple.</p>

<ul>
<li>Leave nothing to question</li>
<li>Be as verbose and clear as possible</li>
<li>Make sure everything is in order, step-by-step</li>
</ul>


<p>By following this simple guideline, I feel I&rsquo;m doing more of a service to the reader than if I presumed on their level of sophistication with Puppet, Linux/UNIX, Windows, research capability, Google-foo or whatever.</p>

<p>So let&rsquo;s dive in, shall we?</p>

<h2>HAProxy</h2>

<p>Seemingly counterintuitive, now that we&rsquo;ve done a split install, I want to next install the HAProxy we will use as a Load Balancer on the additional compilers.  By installing this first, we can utilize Puppet to install the HAProxy, and manage them automatically rather than doing a lot of ad-hoc work.</p>

<p>Also, by doing the proxy first,  the prerequisites are satisfied in their proper order, the Load Balancer exists before configuring additional compilers (to be able to utilize the dns_alt_names for the load balancer along with the compilers) and to have the GitLab in place and hosting the control_repo before turning on and configuring Code Manager.</p>

<h3>Hardware</h3>

<p>In the initial hardware list, I included a node called &ldquo;Compile Master&rdquo;.  This node  looked like:</p>

<p><img src="http://cvquesty.github.io/images/compile_master_specs.png" alt="Compile Master Specs" /></p>

<p>This node may seem like overkill, but disk and memory are cheap.  If you are scaling at this level, its better to not have to reinstall your Load balancer later. Keep in mind, you don&rsquo;t have to use HAProxy and can use a corporate Load Balancer here, but its configuration is outside the scope of this tutorial.</p>

<p>Once you&rsquo;ve provisioned the load balancer, ssh to the node as the root user, and use the &ldquo;frictionless installer&rdquo; to add your Puppet agent.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -k https://master.example.com:8140/packages/current/install.bash | bash</span></code></pre></td></tr></table></div></figure>


<p>When the client is fully installed, retrieve the Enterprise Console from your browser, and navigate to Nodes | Classification | Unsigned Certificates and select &ldquo;Accept All&rdquo;.  Finally, ssh to the instance as the root user and run <strong><em>puppet agent -t</em></strong> to finish the setup.</p>

<h2>Configure the Load Balancer</h2>

<p>At this point, the node is provisioned and you have a Puppet agent running on it, but you have as of yet not configured the HAProxy Load Balancer for use in the environment. The load balancer will be necessary to have in place prior to adding compile masters to your existing split installation. The following instructions guide you through setting up the HAProxy load balancer.</p>

<ol>
<li><p>SSH to the Puppet Master as root.  <em>(<strong>master.example.com</strong> in our list)</em></p></li>
<li><p>Install the HAPRoxy Forge Module on the master</p></li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>puppet module install puppetlabs-haproxy</span></code></pre></td></tr></table></div></figure>


<p><br>
    <em>leave your root console open while performing steps 3-6</em></p>

<ol>
<li><p>Retrieve the Enterprise Console in your browser</p></li>
<li><p>Select <strong>Nodes</strong> | <strong>Classification</strong></p></li>
<li><p>Create a New Classification Group called &ldquo;<strong>Load Balancer</strong>&rdquo;</p></li>
<li><p>Select the new group from the list and pin the node &ldquo;<strong>compiler.example.com</strong>&rdquo; into the new group.</p></li>
<li><p>In your open SSH session to <strong>master.example.com</strong>, create the profiles module to hold the configuration for HAProxy</p></li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /etc/puppetlabs/code/environments/production/modules
</span><span class='line'>
</span><span class='line'>mkdir -p profiles/manifests
</span><span class='line'>
</span><span class='line'>cd profiles/manifests</span></code></pre></td></tr></table></div></figure>


<ol>
<li><p>Once you have changed to the profiles/manifests directory, create the loadbalancer.pp manifest.</p></li>
<li><p>Follow the documentation <a href="https://forge.puppet.com/puppetlabs/haproxy/readme">here</a> to configure HAProxy. When complete, the loadbalancer.pp manifest should resemble the following with IPs corrected for your particular instance:</p></li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Load Balancer Profile
</span><span class='line'>class profiles::loadbalancer {
</span><span class='line'>
</span><span class='line'>  class { 'haproxy': }
</span><span class='line'>
</span><span class='line'>  # Main Proxy Listener
</span><span class='line'>  haproxy::listen { 'compiler.example.com':
</span><span class='line'>    collect_exported =&gt; false,
</span><span class='line'>    ipaddress        =&gt; $::ipaddress,
</span><span class='line'>    ports            =&gt; '8140',
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  # First Load balanced Compile Master
</span><span class='line'>  haproxy::balancermember { 'compiler1.example.com':
</span><span class='line'>    listening_service =&gt; 'compiler.example.com',
</span><span class='line'>    server_names      =&gt; 'compiler1.example.com',
</span><span class='line'>    ipaddress         =&gt; '10.0.1.24',
</span><span class='line'>    ports             =&gt; '8140',
</span><span class='line'>    options           =&gt; 'check',
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  # Second Load Balanced Compile Master
</span><span class='line'>  haproxy::balancermember { 'compiler2.example.com':
</span><span class='line'>    listening_service =&gt; 'compiler.example.com',
</span><span class='line'>    server_names      =&gt; 'compiler2.example.com',
</span><span class='line'>    ipaddress         =&gt; '10.0.1.25',
</span><span class='line'>    ports             =&gt; '8140',
</span><span class='line'>    options           =&gt; 'check',
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Once you have created this profile, retrieve the Puppet Enterprise Console in your browser and navigate to <strong>Nodes | Classification | Load Balancer</strong>.</p>

<ol>
<li>Selet the <strong>Classes</strong> tab.</li>
<li>Click the &ldquo;refresh&rdquo; button so the console will pick up your new loadbalancer.pp profile to classify your node with.</li>
<li>Under the &ldquo;Add new Class&rdquo; heading, select <strong>profiles::loadbalancer</strong> from the list that drops down.</li>
<li>Click &ldquo;Add Class&rdquo;.</li>
<li>Select &ldquo;Commit 1 Change&rdquo; at the bottom right of the page.</li>
<li>SSH back into <strong>compiler.example.com</strong> and run <strong>puppet agent -t</strong> to configure the Load Balancer.</li>
</ol>


<p>Your Load Balancer is now prepared to balance traffic to two catalog compilers (<em><strong>catalog1.example.com</strong> and <strong>catalog2.example.com</strong></em>) as listed in the above configuration.</p>

<h3>Notes</h3>

<hr />

<p>I noted when putting together the loadbalancer.pp profile above that I had previously used some REALLY ODD ip addresses in the balancer config.  Why? For the life of me I cannot recall. The original file looked like so:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Load Balancer Profile
</span><span class='line'>class profiles::loadbalancer {
</span><span class='line'>
</span><span class='line'>  class { 'haproxy': }
</span><span class='line'>
</span><span class='line'>  # Main Proxy Listener
</span><span class='line'>  haproxy::listen { 'compiler.example.com':
</span><span class='line'>    collect_exported =&gt; false,
</span><span class='line'>    ipaddress        =&gt; $::ipaddress,
</span><span class='line'>    ports            =&gt; '8140',
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  # First Load balanced Compile Master
</span><span class='line'>  haproxy::balancermember { 'compiler1.example.com':
</span><span class='line'>    listening_service =&gt; 'compiler.example.com',
</span><span class='line'>    server_names      =&gt; 'compiler1.example.com',
</span><span class='line'>    ipaddress         =&gt; '10.0.1.24',
</span><span class='line'>    ports             =&gt; '8140',
</span><span class='line'>    options           =&gt; 'check',
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  # Second Load Balanced Compile Master
</span><span class='line'>  haproxy::balancermember { 'compiler2.example.com':
</span><span class='line'>    listening_service =&gt; 'compiler.example.com',
</span><span class='line'>    server_names      =&gt; 'compiler2.example.com',
</span><span class='line'>    ipaddress         =&gt; '10.0.1.25',
</span><span class='line'>    ports             =&gt; '8140',
</span><span class='line'>    options           =&gt; 'check',
</span><span class='line'>  }
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>In my original implementation I set the ipaddres fields with some odd IP addresses. For info around how to fill those but ,the documentation gives some hints:</p>

<blockquote><p>ipaddresses: Optional. Specifies the IP address used to contact the balancermember service. Valid options: a string or an array. If you pass an array, it must contain the same number of elements as the array you pass to the server_names parameter. For each pair of entries in the ipaddresses and server_names arrays, Puppet creates server entries in haproxy.cfg targeting each port specified in the ports parameter. Default: the value of the $::ipaddress fact.</p></blockquote>

<p>Since I was originally setting these up in Digital Ocean, I used the IP space 159.203.x.x which belongs to Digital Ocean. I am guessing these were the hard IPs on the instances I stood up. Since the documentation above states these are optional, you have two options here.  Either leave those lines out of your config altogether, or manually set them to the IP Address of the instance you&rsquo;re using. Try each and do which works for you.</p>

<h2>Conclusion</h2>

<p>Your HAProxy Load balancer is now complete and ready to take traffic to the additional catalog compiler nodes. In installment IV, we&rsquo;ll begin to add in more components along the way to a fully developed LEI of Puppet Enterprise.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scaling Puppet Enterprise - Part II - Installation]]></title>
    <link href="http://questy.org/blog/2017/04/18/scaling-puppet-enterprise-part-ii-installation/"/>
    <updated>2017-04-18T13:03:21-04:00</updated>
    <id>http://questy.org/blog/2017/04/18/scaling-puppet-enterprise-part-ii-installation</id>
    <content type="html"><![CDATA[<p>Installing Puppet Enterprise has been made remarkably easier as time has gone on. The efforts of Puppet Labs (I still can&rsquo;t get used to simply &lsquo;Puppet&rsquo;) to make the installation as seamless and powerful as possible with the simplest of interfaces has been highly successful.</p>

<p>Many changes have occurred over time to include changing from answer files to a <a href="https://docs.puppet.com/pe/latest/config_hocon.html">HOCON</a> formatted <strong>pe.conf</strong> file containing the various configuration elements you may need to stand up an instance. I somewhat preferred the simple nature of the original answer files, but I can see the sense in moving to HOCON moving forward.</p>

<p><strong>Obtain puppet</strong></p>

<p>Needless to say, you&rsquo;re going to need the Puppet Enterprise package to install from. Unlike Puppet Community, the entire installer is provided as a tarball rather than repo based installations via package management, and requires a little bit of UNIX-y knowhow to get it started, as the Puppet Enterprise Server is only installable on Linux.</p>

<p>When you navigate to the Puppet Download page, you may be required to sign up for a free account if you haven&rsquo;t already. The opening download page is found <a href="https://puppet.com/download-puppet-enterprise">here</a>.</p>

<p>You will be presented with a launch page that contains a &ldquo;Download&rdquo; button.  Click the button, and one of two things will happen.  Either you will be directed to a &ldquo;Thank You&rdquo; page or a page to sign up for an account. As you can see, the &ldquo;Thank You&rdquo; page means you already have an account and are signed in whereas the signup page is self-explanatory. Sign up for an account, and retry the download link.</p>

<p>Once you&rsquo;ve made it to the &ldquo;Thank You&rdquo; page, there are three tabs containing &ldquo;Puppet Enterprise Masters&rdquo;, &ldquo;Puppet Enterprise Agents&rdquo;, and &ldquo;Puppet Enterprise Client Tools&rdquo;. As of this writing, the only supported Puppet Master platforms are RedHat 6 &amp; 7, Ubuntu 12.04, 14.04, and 16.04, as well as SLES 11 and 12.</p>

<p>If you had intentions of running the Puppet Master server on any other platform, here is where you realign your expectations.  :)  I have heard that people have hacked the server to run on other platforms, but since we&rsquo;re dealing with Puppet Enterprise, why would you break support and eliminate warranty?  Pick one of the three and download the tarball for your appropriate platform.</p>

<p><strong><em>NOTE:  If You need legacy versions of PE, you can download those <a href="https://puppetlabs.com/misc/pe-files/previous-releases">here</a>.</em></strong></p>

<p><strong>Installation</strong></p>

<p>For the purposes of this scenario, we will be installing the Puppet Infrastructure for fictional super-mega huge company &ldquo;example.com&rdquo;. I am going to trust you have worked out the DNS/Host file naming structure, and can resolve everything everywhere.  If you cannot, don&rsquo;t comment on the post, as I will make fun of you publicly&hellip; you deserve it.</p>

<p>My assumed setup will be:</p>

<p><img src="http://cvquesty.github.io/images/node_list_example_com.png" alt="Example.com Node List" /></p>

<p><strong><em>Automated</em></strong></p>

<p>The Puppet Enterprise Installer is a GUI web-browser based installer. Puppet has gone through the process of giving you a nice frontend to your installation, and making it dead-easy to perform a monolithic as well as split installation.  For our purposes, though, we will be doing a &ldquo;split&rdquo; installation.</p>

<p><strong>Stand up 3 Nodes with the specifications from the first article in the series as follows:</strong></p>

<p><img src="http://cvquesty.github.io/images/split_node_list.png" alt="Split Node List" /></p>

<p>In my experience, I&rsquo;ve found it much easier to exchange root keys between all three of the above nodes to allow the installer to do all it needs to do on each node. You can, however, decide to set the root password to something temporary to hand to the installer as well (and many people opt for this) and then return root&rsquo;s password to your site default.  In any event, all the machines should be able to resolve themselves and each other by name and root should be able to freely ssh between them either via shared keys (easiest) or password.</p>

<p><strong>Transfer the package to the Puppet Master node:</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scp -rp puppet-enterprise-2015.3.2-el-7-x86_64.tar.gz root@master.example.com:/root/</span></code></pre></td></tr></table></div></figure>


<p>Once the package is on the destination machine, you should connect to the machine to work with the package on-box:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ssh root@master.example.com</span></code></pre></td></tr></table></div></figure>


<p>which places you in the root user&rsquo;s home directory where you copied the package.</p>

<p><strong>Extract the Package</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tar -zxvf puppet-enterprise-2015.3.2-el-7-x86_64.tar.gz 
</span><span class='line'>cd puppet-enterprise-2015.3.2-el-7-x86_64</span></code></pre></td></tr></table></div></figure>


<p><strong>Run the Installer</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>./puppet-enterprise-installer</span></code></pre></td></tr></table></div></figure>


<p>You will receive a text prompt that states:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>??Install packages and guided install [Y,n]</span></code></pre></td></tr></table></div></figure>


<p>Simply press &ldquo;Y&rdquo; or the [Enter] key and the GUI portion of the installation will begin.</p>

<p><strong><em>GUI Installer</em></strong></p>

<p>Once you have started the Installation, the Puppet Enterprise Installer will perform some preparatory steps and then launch an installation interface on your master node on port 3000. To access this interface, you can bring it up in the web browser of your choice at:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>https://master.example.com:3000</span></code></pre></td></tr></table></div></figure>


<p>Navigate to this interface in your Internet browser. When you first arrive at the GUI installer, simply click the &ldquo;Let&rsquo;s Get Started&rdquo; button. On the next page, select &ldquo;Split&rdquo; to begin the Split Installation.The Puppet Enterprise Installer will present you with a GUI questionnaire to fill out regarding your environment. The following is that process in order by section.</p>

<h2>Puppet Master Component</h2>

<ol>
<li><p>Choose the &ldquo;Install on this Server&rdquo; radio button.2. Enter the name of your Master in the <strong>Puppet Master FQDN</strong> text box. (e.g. master.example.com) 3. Enter all appropriate names for your master in the <strong>Puppet Master DNS Aliases</strong> text box.4. Select the &ldquo;Enable Application orchestration&rdquo; check box.## PuppetDB Component</p></li>
<li><p>Enter the hostname of your PuppetDB Node in the <strong>PuppetDB Hostname</strong> text box. (e.g. puppetdb.example.com)</p></li>
<li>Change no other selections under the remainder of the items for this section.</li>
</ol>


<h2>PE Console Component</h2>

<ol>
<li>Enter the hostname of your Puppet Console in the Console Hostname text box. (e.g. console.example.com)</li>
<li>Change no other selections under the remainder of the items for this section.</li>
</ol>


<h3>Database Support</h3>

<p>No changes are needed to is section. Simply leave &ldquo;Install PostgreSQL on the PuppetDB host for me&rdquo; selected.</p>

<h3>Console &lsquo;admin&rsquo; User</h3>

<p>Enter the password you would like to use for the Puppet Enterprise console once your installation is complete in the final text box.</p>

<h2>Final Considerations</h2>

<p>After completing the final section, click the &ldquo;Submit&rdquo; button, and the Puppet Enterprise Installer will present you with a confirmation page for you to review before commencing the installation based on the configuration elements you just provided to the installer.</p>

<p>If everything is to your satisfaction, click the &ldquo;Continue&rdquo; button and the Puppet Enterprise Installation will begin.The Installation progress summary will continue to update you as to the progress of the installation. If you would like to see logging &ldquo;as it happens&rdquo;, you can click the &ldquo;log view&rdquo; button to see that in real time. If you would like to switch back to the summary, simply click the <strong>Summary</strong> button.After what is roughly 10-15 minutes of installation and configuration, the installer will have completed all its work, and you will be presented with a button at the bottom of the progress screen you have been viewing that says: &ldquo;Start Using Puppet Enterprise&rdquo;.  Click that button, and the installer will redirect you to the PE Console login screen.  Enter the admin credentials you created earlier, and you are ready to begin working with the console as needed.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scaling Puppet Enterprise]]></title>
    <link href="http://questy.org/blog/2017/04/17/scaling-puppet-enterprise/"/>
    <updated>2017-04-17T16:06:11-04:00</updated>
    <id>http://questy.org/blog/2017/04/17/scaling-puppet-enterprise</id>
    <content type="html"><![CDATA[<p>In my former life as a consultant, I had to install all manner of configurations of Puppet for clients.  Some were small and some were large, but none were <strong><em>VERY</em></strong> large.  One of the big things I was finding back then was there just wasn&rsquo;t a lot of publicly available information regarding doing a full install and scaling it large.</p>

<p>So, I took some &ldquo;research time&rdquo; on my own and started to build out the configurations according to Puppet Labs' (at the time&hellip; now just &ldquo;Puppet&rdquo;) documentation. The problem I was having was that the docs wouldn&rsquo;t ever lead me to a successful install following a chronolgical set of steps. I had to click into subpages, jump over to sub-sub configurations, and then jump back to the main docs to follow yet another trail down until I reached the end&hellip;lather, rinse, repeat.</p>

<p><strong>Some Caveats..</strong></p>

<p><strong>First</strong>, this is probably no longer a good &ldquo;HOWTO&rdquo; unless you&rsquo;re installing an older Puppet Enterprise. It was created between 2015.2 and 2016.x, and likely has some amount of artifacting related to those versions.</p>

<p><strong>Second</strong>, I&rsquo;m going by docs I&rsquo;ve recorded for my own use.  I wrote these as mentioned above through prototyping, tearing it down, starting again, and literally doing the entire install over and over until it worked &ldquo;as advertised&rdquo;. A lot of this was really just ordering things the right way, and finding documentaiton for various pieces online at Puppet&rsquo;s documentation site as well as blogs, conversations, and plain old trial and error. I certainly can&rsquo;t warrant anything to anybody for any reason. As with most open source/creative commons assets, &ldquo;it works for me, hope it works for you, and if it doesn&rsquo;t, sorry about that.&rdquo;</p>

<p><strong>Finally</strong>, I hope to use this as the springboard to start brain-dumping all my old notes, conversations, ideas, and other prototyping I did in my home lab.  There&rsquo;s still a fair amount of documentation I cannot use or touch because they belong to my former employer or Puppet Labs, so some things may be less than clear and usually because I&rsquo;m dancing around an NDA, noncompete, or just plain being a nice guy. If I inadvertently reveal something I shouldn&rsquo;t, chances are it could disappear without a trace, but I&rsquo;ll still make a note that I removed something, and try and replace whatever it is with published docs.</p>

<p>In short: I want to help the community, but I&rsquo;m walking a tightrope here, so please be kind.</p>

<p><strong>Format</strong></p>

<p>I hope to start easy with a decision making process for installing Puppet, how to choose a method, think about scale, and will likely have quite an opinionated view at times.  Once PE is installed, we&rsquo;ll add compilers, scale postgres, etc. but for starters, I hope to just have the following:</p>

<p>PE Master (MoM)<br>
Puppet DB<br>
PE Console<br>
HA Proxy Node for Compilers<br>
Two Catalog Compilers<br>
One ActiveMQ Hub<br>
Two ActiveMQ Spokes<br>
Two Agent Nodes for testing<br></p>

<p>I know that&rsquo;s quite a number of nodes to get started with, but this after all a large environment infrastructure, and we want to scale big.</p>

<p><strong>Required Nodes</strong></p>

<p>To put together all the required components for a good large installation, I&rsquo;ve settled on the below specs. You can change those as you see fit, but note that some of the disk space requirements and related were due to Puppet&rsquo;s documented requirements <em>at the time</em>.  YMMV, of course, but this is what I consider to be a base level installation if you intend on scaling into the multiple tens of thousands of nodes. Be sure that if you&rsquo;re going to size this down that you&rsquo;re still meeting Puppet&rsquo;s needs in regards to memory, cores, and disk.  <em>(for a current listing of Puppet&rsquo;s reuqirements, you can look <a href="https://docs.puppet.com/pe/latest/sys_req_hw.html">here</a> for more information.)</em></p>

<p><img src="http://cvquesty.github.io/images/prerequisites.png" alt="Puppet_Prerequisites" /></p>

<p>In addition, you&rsquo;ll need to be aware of firewall requirements for such an installation.  Puppet has documentation regarding firewall configurations and needed ports at their website <a href="https://docs.puppet.com/pe/latest/sys_req_sysconfig.html#for-large-environment-installations">here</a>, but I&rsquo;ll insert the image and recount the requirements here.</p>

<p><img src="http://cvquesty.github.io/images/lei_port_diagram.png" alt="Firewall_Ports" /></p>

<p>In short:</p>

<p><img src="http://cvquesty.github.io/images/firewall_ports.png" alt="Firewall_Ports" /></p>

<p>This is a close approximation to what you need to know.  Detailed charts found in the above links, and a &ldquo;point-by-point&rdquo; port and use list is available to review.</p>

<p>In short, I&rsquo;ve found it easiest to have all PE components on the same VLAN with no restrictions between them. If you are going to have a local firewall turned up on each node, you&rsquo;ll need to manage all the above communications as you see fit, but for the serving infrastructure (if in a secure environment, of course) you can likely drop host firewalls in favor of corporate ones.  In short, make it as easy on yourself as you see fit while balancing that toward your corporate security policy.</p>

<p>Finally, make sure DNS and NTP are all ready to go.  I can&rsquo;t tell you the number of times I&rsquo;ve had major issues trying to get all this working, and NTP was off, or DNS didn&rsquo;t propagate as expected (it&rsquo;s always DNS, right?) or some other similar seemingly unrelated piece was not restarted or some such. Just make sure that all nodes resolve to their respective FQDN from all nodes. Obviously, the easiest way to do this is to simply put them all in DNS. You <strong><em>can</em></strong> manage the host files manually, but why would you want to do that?</p>

<p>If you&rsquo;re at this point and all ready to go, look to the next entry to get started.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Changes Are Afoot]]></title>
    <link href="http://questy.org/blog/2016/08/31/changes-are-afoot/"/>
    <updated>2016-08-31T10:33:00-04:00</updated>
    <id>http://questy.org/blog/2016/08/31/changes-are-afoot</id>
    <content type="html"><![CDATA[<p>After much thought and consideration, I&rsquo;ve terminated my employ with ShadowSoft. I was travelling nearly every week all over the US, and not with my family as much as I&rsquo;d like. Unfortunately, this role was a 70% travel role, and our youngest needed Daddy home.</p>

<p><img src="http://cvquesty.github.io/images/paypal.png" alt="PayPal" /></p>

<p>As luck would have it, a totally awesome telecommute FT/Perm option came up with PayPal, and I accepted rather excitedly, and began that role today.  After some getting acquainted with my new duties, you should be seeing/hearing more from me on the Puppet front soon.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Travel Hiatus]]></title>
    <link href="http://questy.org/blog/2016/06/13/travel-hiatus/"/>
    <updated>2016-06-13T10:06:29-04:00</updated>
    <id>http://questy.org/blog/2016/06/13/travel-hiatus</id>
    <content type="html"><![CDATA[<p><strong>Out</strong></p>

<p>Just a quick update for you all. $work decided at some point that one of the things I was specifically hired for (blogging in the community) some sort of way &ldquo;gives away the farm&rdquo; in regards to Puppet, Puppet Consultation, and related items. As such, I&rsquo;ve been asked not to blog publicly regarding items we deliver as services.</p>

<p>:-(</p>

<p>It likely doesn&rsquo;t matter that much, as I&rsquo;m travelling more than I have in years, and am on-target to exceed 145k miles by Summer&rsquo;s end.  As a result, I will be laying off until I can regroup and find more time.</p>

<p>Sorry, but as they say &ldquo;them&rsquo;s the breaks&rdquo;.</p>

<p>If you need me, you can always find me on the Puppet community Slack Channel #puppet, and my nick there is @cvquesty.</p>

<p>Look for interesting news from me soon.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PuppetConf 2015]]></title>
    <link href="http://questy.org/blog/2015/10/27/puppetconf-2015/"/>
    <updated>2015-10-27T22:11:28-04:00</updated>
    <id>http://questy.org/blog/2015/10/27/puppetconf-2015</id>
    <content type="html"><![CDATA[<p><strong>PuppetConf Portland</strong></p>

<p><img src="http://cvquesty.github.io/images/logo.jpg" alt="PuppetConf" /></p>

<p>Ahh, Portland! What a great place to have PuppetConf this year. The home of Puppet Labs and all its varied food, drink, and other unnamed consumables give Portland a vibe like no other.</p>

<p>From the hipster eateries to the burger dives around town, Portland offered something for everyone.</p>

<p>My week began by arriving a tad early for the Puppet Certifed Consultant training day. On the way in, I passed this little beauty right here:</p>

<p><img src="http://cvquesty.github.io/images/mountain.jpg" alt="Mountain" /></p>

<p>I had forgotten just how beautiful the Pacific Northwest can be.</p>

<p>In our meetings before the conference began in earnest, we talked about things announced and things as yet unannounced, and essentially just learned how to be better consultants and puppeteers. It was nice to be able to ask the questions that arise from time to time of the “big boys” (Gary Larizza, Zak Smith, etc.) and get first-hand accounts on how to do better.</p>

<p>There were quite a number of really cool talks on various upcoming tech, that I had quite a bit of opportunity to take notes and build upon knowledge I’d already gained.</p>

<p><strong>Keynotes!!</strong></p>

<p><img src="http://cvquesty.github.io/images/keynote_wide.jpg" alt="KeynoteWide" /></p>

<p>Next up was the PuppetConf Keynotes for the first day that usually contains Luke Kanies' annual Puppet conversation. Where they’ve been, what they’re doing, and the roadmap forward was fodder for Luke’s talk, and you can find the complete Keynote <a href="https://puppetlabs.com/presentations/puppetconf-2015-lukes-keynote-address">here</a>:</p>

<p>The big synopsis I can give is all about application automation. For you Puppeteers out there, just think of the relationships between the File|Package|Service component “types” and apply that to application components (db, web, container, Java App, etc.) and you get the gist. Very cool, very powerful, and very near. Be looking for Puppet Enterprise 2015.3 to drop in the very near future. I’ll certainly have soe blog things to say when that happens.</p>

<p>I appreciate, once again, my employer <a href="http://shadow-soft.com">Shadow Soft</a> sending me out to PuppetConf to be the best engineer I can be, and learn all the new tricks and tools at my disposal while on the road.</p>

<p>Look for me to pick up where I left off with basic tools, and a revamp of my early-on configuration tutorial for Puppet Community with the new tools and features in Puppet 4 soon.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Some Software Releases and Forge Stuffs]]></title>
    <link href="http://questy.org/blog/2015/07/28/some-software-releases-and-forge-stuffs/"/>
    <updated>2015-07-28T10:11:55-04:00</updated>
    <id>http://questy.org/blog/2015/07/28/some-software-releases-and-forge-stuffs</id>
    <content type="html"><![CDATA[<p><strong>Salutations</strong></p>

<p>First of all, hello from sunny and hot New York. I’m on engagement for my company doing some Puppet goodness in the Northeast.</p>

<p>In my downtime at night, Ive been wrapping up some work I’ve had on my plate for awhile (my Puppet module destined for the Puppet Forge) and generally studying for various topics if for no other reason than to get better.</p>

<p>As I was preparing my new module, Puppet releases the Puppet4 Enterprise release that now follows new semantic versioning schemes, and follows the scheme:</p>

<p>Puppet Enterprise YYYY.VV</p>

<p>where “V” is version number.</p>

<p><strong>Puppet Enterprise</strong></p>

<p>As many of you know, I’ve been maintaining a project for Puppet prototyping and working with development and testing over a Vagrant instance for some time. I created what I have because I needed something I could share with customers to help facilitate coding and iteration without their touching the production instance unless absolutely necessary.</p>

<p>Thus, my projects were born.</p>

<p><strong>Vagrants. Vagrants Everywhere.</strong></p>

<p>In a nutshell, I configure a Vagrant environment on a modern OS. I create 4 nodes. One is the Puppet Master itself and the remaining three are Puppet agents that check in with the master and are in three faux envirnments: “Production”, “Testing”, and “Development”. As a result, you can code for DEV and iterate the heck out of it. Once you like it, you can merge up the tree into testing and finally to production.</p>

<p>These releases have been followng the format:</p>

<p>[OSNAME]|[VERSION NUMBER]-[PUPPET][PUPPETVERSION]</p>

<p>So, for instance, a release for CentOS5 running Puppet Open Source 4.0 would look something like this:</p>

<p>centos5-po4</p>

<p>Make sense?</p>

<p>Well, as of today, my new release will be CentOS7 with Puppet Enterprise 2015.2. It can be found here.</p>

<p><strong>Puppet Forge Module</strong></p>

<p>I’ve also been working on two separate modules for the forge here recently. The other I’ve been working on longer, but this one was just ready first. I call it “PuppetDev”.</p>

<p>The idea behind the PuppetDev module is that sometimes a company who needs to do Puppet Development has corporate policy against using a tool like the Vagrant instances (i.e. virtualization on the desktop) mentioned above. As a result, the company often will set up a centralized development host that people can login to for developing Puppet code.</p>

<p>Often times, though, when starting out, a user can feel overwhelmed at the simple command line in front of them, and even if they get into an editor, they may not know where to start with syntax highlighting and the like.</p>

<p>It was from this need PuppetDev was born. You simply apply the module to a node, and supply the user/group you want to apply the module to as parameters, and it whirls away and sets up their development environment.</p>

<p>Among the toys they get with the release are syntax highlighting, an easier to read colorscheme, a Vim plugin infrastructure, and all manageable by the Puppet Administrator for the site.</p>

<p>I know it’s a rather narrow use case, but there it is… feel free to check it out here, or if you’re so inclined, on your Puppet master you can simply run puppet module install cvquesty/puppetdev.</p>

<p>That’s all I have for this update, but hope to be a little more active here shortly with my next Forge module.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Organizing Your Hierarchy Equals Pain]]></title>
    <link href="http://questy.org/blog/2015/05/15/organizing-your-hierarchy-equals-pain/"/>
    <updated>2015-05-15T17:48:59-04:00</updated>
    <id>http://questy.org/blog/2015/05/15/organizing-your-hierarchy-equals-pain</id>
    <content type="html"><![CDATA[<p><strong>The Pain Point</strong></p>

<p>One would think after reading Gary Larizza’s blog that I wouldve come away with the idea that Hiera presents a few issues as it solves a ton… but no. I had to go and think it was easy, fly off half-cocked and try and tackle a big issue or two, unprepared mind you, and here I am… re-discovering what humility should be like.</p>

<p><strong>The Problem</strong></p>

<p>Hiera looks simple. Disarmingly simple. However, the pain doesn’t come in just looking at a nice, default hiera.yaml:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>---
</span><span class='line'>:backends:
</span><span class='line'>- yaml
</span><span class='line'>:hierarchy:
</span><span class='line'>- “%{clientcert}”
</span><span class='line'>- “%{environment}”
</span><span class='line'>- common
</span><span class='line'>:yaml:
</span><span class='line'>:datadir: “/etc/puppetlabs/puppet/environments/%{environment}/hieradata”</span></code></pre></td></tr></table></div></figure>


<p>We can simply look at this and see the wonderful simplicity of how the yaml file is laid out, the ease of adding more hierarchies from which to gain data, and even expand the model to include subdirectories and all sorts of interesting methods of organizing and abstracting our code into usable, organized chunks.</p>

<p>Not so fast.</p>

<p>The real issues begin when you’re dealing with a customer. All too often I find that even they aren’t entirely sure what’s going on in their very own environment, and pushed hard, will actually argue among themselves as to just how everything works. Scary.</p>

<p>The main thing we find ourselves doing is figuring out that last mile… What precisely is the role of machine X in this environment? Ask two people separate from each other, and they’ll likely give different (sometimes remarkably so) answers. Get them together and they may quibble a bit, but generally get to consensus.</p>

<p>This is REALLY, REALLY important. If the folks you’re trying to help aren’t 100% sure exactly:</p>

<p>What a machine does
How a machine is built
What it’s role in the organization is
You’ve got some real issues.</p>

<p><strong>This Doesn’t Suck</strong></p>

<p>Often, engineers look at the Hiera configuration file in a vacuum (so much for the suck joke). Not permanently but definitely independently at first. Then, as the engagement pushes on to defining Roles and Profiles for the site, you have this “oh crap” moment, and throw back to the hierarchy and then start modifying lookup layers, but then jump forward to the profiles where the lookups are (or should be, anyhow) and realize they assumed a different hierarchy. Then, jump back to the hiera.yaml, make changes, then back to the profiles and repeat the process and then finally to the component modules, and remediate any assumptions you made on everythin gyou just changed. Uh oh. Wasted time.</p>

<p>I’ve started working through “all the things” and have come up with a mechanism that works well for me. Hopefully you can gain some mileage from it as well.</p>

<p><strong>First Thing’s First</strong></p>

<p>Some would argue you should do the Hierarchy first while others would argue you do the Profiles first. However, I’ve found that parsing out all the business logic with the team gains a remarkable amount of runway for you to start. Why?</p>

<p>Systems engineers are techno-nerd types. The nuts &amp; bolts, configs and the like are their prime concern and more often than not, they view the site atomically. They can tell you with great detail precisely what each and every machine has installed on it (often…not always), but generally know what the IT ROLE of a node or collection of nodes is for. Further, they can tell you who requested it, what kind of storage may be connected, what business group out there it satisfies the needs of, but with a startling amount of frequency, cannot tell you the BUSINES ROLE of the node.</p>

<p>Q: What is it?<br>
A: A web server.<br>
Q: What’s it’s purpose?<br>
A: To serve web documents, duh!<br>
Q: No, no… what’s it’s business purpose?<br>
A: To make money?<br>
Q: No, no… If you were to give it one overarching purpose, one reason for existing, what would it be?<br>
A: Oh… ummm… I never thought of that before.<br></p>

<p>You’d be surprised just how often you arrive there with pretty much everyone.</p>

<p>As a result, if you wait until mid-engagement to reach this point, (or at least the middle of the writing phase), you’ve got a fair amount of backtracking, and even refactoring to do before you regain some sense of normalcy and can push forward.</p>

<p><strong>Most Specific to Least</strong></p>

<p>As has been said many times and in many ways, your MOST specific designation should always come first. For instance, what is the MOST atomic level of abstraction? Well, the node itself, of course, so the %{clientcert} designator suffices for that.</p>

<p>Well, what’s next? That depends on you. You might have a location to think of (is this data center on the east or west coast, US or Asia). That’s highly broad… maybe not. It might be environment (such as DEV, TEST, PROD). Again, this is custom to you, and that might still be overly broad and you need to find a happy place between clientcert and environment. Only you can tell me that when I’m standing in front of you, so I generally refrain until I can get the layout of your site.</p>

<p>For instance, one customer had clientcert, then location, then environment. That way, items unique to the data center the nodes were in would get handled first, and then things that were environment unique (regardless of location – more broad) could get handled next. See? Custom to them and the way they do business or are arranged technologically.</p>

<p>I “borrowed” the name for this post from Bill Engvall to illustrate a point, that if you just run off to development with no prior knowledge of the things Hiera works with, you will encounter the pain of refactoring at what is most likely the component module level and then the profile level as well. If you’ve ever had to do it, and then do it on a time crunch, truly you have felt the pain.</p>

<p>Avoid it. Think before you act.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puppets.. Puppets Everywhere]]></title>
    <link href="http://questy.org/blog/2015/04/30/puppets-dot-puppets-everywhere/"/>
    <updated>2015-04-30T12:52:27-04:00</updated>
    <id>http://questy.org/blog/2015/04/30/puppets-dot-puppets-everywhere</id>
    <content type="html"><![CDATA[<p><strong>3.8 is Here!!!</strong></p>

<p>That’s right, kids. PE 3.8 has dropped, and it is quite tasty. Some highlights:</p>

<p><strong>AWS Module Now a Supported Module</strong></p>

<p>As simple as that sounds, it’s huge. Being able to stand up multiple, tens, even hundreds and thousands of servers into AWS at once with Puppet is a great thing, but to have the module supported by Puppet Labs Support is even better.</p>

<p><em>Docker Containers??</em></p>

<p>Indeed. The Node manager now “gets” Docker containers and you can provision from bare metal as needed. Once the provisioning is done, it hands directly off to Puppet to execute the configuration portion of your run. Seet, sweet sauce right there.</p>

<p><em>Bare Metal</em></p>

<p>You’ve always been able to foray into the world of bare metal provisioning, but now it too is supported for you. You can stand up OSes, hypervisors, and then hand those off into the config run using Razor. Razor is now core to PE and also supported by the Puppet Labs Support Team.</p>

<p><em>Code Management</em></p>

<p>A long time coming, you can also manage code deployment to your Puppet Master using r10k, installed by default. Newly dubbed the “Puppet Code Manager”, r10k remains a command line tool, but I hear rumblings there may be some GUI juice on the horizon for this.</p>

<p><strong>Deprecations</strong></p>

<p>As with any release, some Puppet Enterprise features are going the way of the Dodo Bird. Some expected, some surprising, Puppet Enterprise’s landscape is certainly changing.</p>

<p><em>Cloud Provisioner</em></p>

<p>Long decried as a weak part of the PE infrastructure, the newly announced AWS Supported Module renders it redundant, and as such is removed from the shipping product’s default installation. Of course, if you have a large infrastructure that leverages the Cloud Provisioner, you can continue to use it by installing it into PE separately.</p>

<p><em>Live Management</em></p>

<p>Live management, a long-standing feature of the Enterprise Console, is now also deprecated. Of course, with the new code management features “baked-in” to Puppet Enterprise through r10k, Live Management is somewhat redundant. However, Puppet Labs notes that they will be releasing improved resource management functionality in future releases. If you need Live Management, then just as you can with the Cloud Provisioner, you can turn it on as well in the 3.8.0 product.</p>

<p><strong>Compatability</strong></p>

<p>Finally, some older versions of supported OSes are no longer so, and the list is as follows:</p>

<p>centos-5-i386<br>
centos-5-x86_64<br>
centos-6-i386<br>
debian-6-i386<br>
debian-6-x86_64<br>
debian-7-i386<br>
debian-7-x86_64<br>
oracle-5-i386<br>
oracle-5-x86_64<br>
oracle-6-i386<br>
redhat-5-i386<br>
redhat-5-x86_64<br>
redhat-6-i386<br>
scientific-5-i386<br>
scientific-6-i386<br>
sles-11-i386<br>
ubuntu-1004-i386<br>
ubuntu-1004-x86_64<br>
ubuntu-1204-i386<br>
ubuntu-1404-i386<br></p>

<p>I’m sure you may have some of these in your infrastructure, but they’re usually the result of a vendor application’s supported platforms. If so, you may wish to communicate back upstream to your various vendors, because when you upgrade PE, these go away for you.</p>

<p><strong>Try It Out</strong></p>

<p>As usual, I’ve already created a Vagrant instance to allow you to test and work with the new PE, testing out your existing code on the new platform. Check it out on my GitHub here.</p>

<p>Let me know if you find any issues, and happy Puppeting!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Some Coding Work of Late...]]></title>
    <link href="http://questy.org/blog/2015/03/26/some-coding-work-of-late-dot-dot-dot/"/>
    <updated>2015-03-26T11:26:16-04:00</updated>
    <id>http://questy.org/blog/2015/03/26/some-coding-work-of-late-dot-dot-dot</id>
    <content type="html"><![CDATA[<p><strong><em>I AM NOT A CODER</em></strong></p>

<p>I know that sounds a little silly with all the Coderwall links over there in the sidebar, but I’m not.</p>

<p>I got into this business as a lowly PC building guy, and worked my way into systems administration through light consulting. A necessary evil of the day was tweaking an autoexec.bat &amp; config.sys to release as much memory to the user as was possible for applications. (This was long before Win95, FYI)</p>

<p>As progress and learning would have it, I landed myself a systems administration job and began to grow. Here, 22 years later, back to consulting (but on a much larger scale), I look back on my professional career and see and realize that there’s a LOT of code behind me. Perl, BASH, ksh, HTML, PHP, light Ruby, old DOS debug scripts, Puppet DSL, Expect scripting… tons of it. All encountered and fleshed out in the context of systems engineering and/or management over the years as situations and needs arose.</p>

<p>Fast forward to today. The juxtapostion of Development, QA, and Operations into one big hairy hard-to-define (but getting clearer) term known as “DEVOPS” is the landscape a new admin comes into, and he or she learns from the very beginning the principles of placing infrastructure definition into code, and working as a developer to enhance and automate the hard infrastructure of the operations world.</p>

<p>I say all that to say this… I’ve got some new releases on my GitHub I’d like to share with you to help you out while navigating in this world of DEVOPS. If you have to call me a coder because of it, I may frown, but it is what it is.</p>

<p><strong><em>Vagrant/Virtualbox Fun</em></strong></p>

<p>As I’ve been slowly revealing through my series in past months, there’s a lot of tools out there for working with Puppet and there’s a ton of the same to prototype for your company’s environment. One of these is Vagrant, and it has the ability, in a huge way, to help you automate the setup and teardown of sample infrastructures to work with your Puppet code in. I’ve just updated and released a few of these, and I want to tell you about them.</p>

<p><strong>Vagrant with CentOS 6.5 and PE 3.7.1</strong></p>

<p>If you look here, you’ll find my current project I use with customers. This is a Vagrant instance that turns up a 4-VM environment including a PE Master, a DEV, TEST, and PROD VM running the PE agent, Enterpise Console, Directory Environments pre-configured, r10k configured, and a simple set of Puppet Modules to get you started.</p>

<p>Most commonly, I share those with customers, coworkers, and community folks to get them started coding right away, and to have a platform with which to teach them how to deploy, merge, and promote code through an environment in a smaller version of wat they might already have in their company. This is the enviroment I spoke about at the Atlanta Puppet User’s Group last year in its current iteration.</p>

<p><strong>Vagrant with CentOS7 and PE 3.7.2</strong></p>

<p>Similarly, you can find a CentOS7 + PE 3.7.2 project here. Much like the above, you get the latest of PE with CentOS7 to help your prototyping over a more current OS.</p>

<p><strong>Vagrant with CentOS7 and Puppet OSS 4</strong></p>

<p>If you look up the word “experiemental” in the dictionary, this project right here is linked as an example.</p>

<p>I’ve gotten a very rudimentary working setup of a Master and one agent to install completely and autosign, and haven’t even scratched the surface of all the new goodies in Puppet 4. As Puppet 4 is still in Beta, this is not recommended in any way for any reason at any time for you to use for any purpose. :)</p>

<p>My hope here is to prepare myself for the PE4 features long before they’re released. I hope to work on getting directory environments and r10k working for this only to have a base from which to rapidly develop for PE4 when it’s released. <strong>EXPECT THIS ONE TO GO AWAY IN FAVOR OF THE NEW PROJECT.</strong></p>

<p><strong>YOU HAVE BEEN WARNED</strong></p>

<p>I hope these projects assist you in rapidly creating a platform and developing for Puppet. If you have any questions, don’t hesitate to contact me via jsheets@shadow-soft.com, quest@questy.org, or one of the many other social media nexii you have available to you.</p>

<p>As always, these are in active, deep development. If you’ve got some Vagrant chops and/or want to contribute in any way, feel free to do pull requests, and I’ll integrate changes as soon as I’m able between customer engagements and/or other duties I may have here at Shadow-Soft.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building Your Toolbox]]></title>
    <link href="http://questy.org/blog/2015/03/26/building-your-toolbox/"/>
    <updated>2015-03-26T09:15:18-04:00</updated>
    <id>http://questy.org/blog/2015/03/26/building-your-toolbox</id>
    <content type="html"><![CDATA[<p>I know it’s been since June we’ve worked on the Puppet Development series, but as work goes, so go I, and as I go, so delays the blog. :)</p>

<p><strong>Recap</strong></p>

<p>We started our journey with a reintroduction to Vim. As strange as it sounds, often times we techno-guys take for granted that people coming into the DEVOPS space are well versed in all these things, and overlook remediating the basics. We covered Vim basics and switching between command and insert mode as well as linking you to some good cheat sheets to help you beef up your vim-fu.</p>

<p>Next, we covered Vim plugins for syntax highlighting and just general code visualization so we can see visually when our code editing has issues and needs to be fixed.</p>

<p>The next article centered on revision control in general, but Git in particular. In addition to using Git, we also covered that amazing tool GitHub and how to get registered for it, create your own repos, and how to work with repos from your local command line as well and I linked you some excellent resources on Git to expand your knowledge of the Git world and become proficient and fluent in its use.</p>

<p>Finally, we got around to Vagrant and I gave you a simple tour of Vagrant to be familiar with the tool and what it does for you. We stood up a Vagrant instance and saw how we could destroy and re-provision that precise same instance with only a shell command, and saw the power of automated provisioning at work right on our own node.</p>

<p><strong>Reasons</strong></p>

<p>Like the social media world will tell you, I did it because reasons. In short, I wanted you familiar with all these separate tools as we start to coalesce them into an integrated whole we can use as our development toolbox.</p>

<p>So, since we’ve got Vim, Git, and Vagrant as needed components, what else might we need to continue pressing forward?</p>

<p>We need to understand <strong><em>Puppet</em></strong> itself.</p>

<p><strong>Puppet - The Product</strong></p>

<p>Puppet, as I’m sure many of you are aware, is simply “configuration management software” produced by Puppet Labs, Inc. Puppet Labs was founded in 2005 by then Systems Administrator/Engineer Luke Kaines to help automate common repetetive tasks encountered in his regular work duties encoutered on a day-to-day basis.</p>

<p>After a few rounds of venture funding and explosive growth of the market segment known as “configuration management”, Puppet Labs has become a market leader in the space, and continues to develop and improve upon the product at a rather aggressive rate.</p>

<p><strong>Configuration Management</strong></p>

<p>If Puppet is “configuration management” software, what is this thing called “configuration management”?</p>

<p>Configuration management as a systems engineering process covers a lot of landscape in its purview. It can mean, speaking generally, a process for maintaining consistency of a product’s performance and can become considerably complex, such as the methodology used to manage miliary weapons systems, IT service management, and other domain models covering civil and industrial engineering.</p>

<p>For the purposes of the technical field of Systems Administration, Engineering, and Automation, however, Configuration Management as a discipline is very well defined. Specifically, the model that covers these areas is Operating System Configuration Management. Certainly, when automating your site you step over into additional disciplines, but at its core, Configuration Management almost always implies that you are working with the primary target of Operating Systems Configuration Management.</p>

<p><strong>IT Automation</strong></p>

<p>Often times, configuraiton management as it is expressed in the realm of operating systems more specifically begins to take on automation as the primary characteristic of the work performed. From provisioning to deployment, automation saves the most time and effort through modeling systems design in a modular fashion, whereby allowing you to apply systems configurations against classes of machines tooled to perform specific types of work.</p>

<p>This paradigm allows for many idioms to be used in the description of the destination machines, and is the primary domain within which products like Puppet operate.</p>

<p><strong>Puppet – the Product</strong></p>

<p>As you peruse the main website for Puppet Labs, you start to see a much larger emphasis and prevalence of “IT Automation” throughout the website–specifically in the area of data center automation. It is important to note that Puppet prefers to work within this space, although it has abilities that stretch into the entire IT lifecycle workflow.</p>

<p>Puppet is comprised of two distinct products: “Puppet Enterprise” and “Open Source Puppet”. As is common in the Open Source space, the distinction between the two is defined primarily by way of the support options available for each.</p>

<p><strong>Puppet Enterprise</strong></p>

<p>The Puppet Labs flagship product is Puppet Enterprise. Puppet enterprise is a fully supported, maintained, and actively developed data-center ready software package designed to enable you to model configurations for your site out-of-the-box. It has an integrated installer, smoothing the installation process, a series of Puppet apps available for use in the Enterprise Console (the Puppet GUI), and for-pay support and licensing options to meet the needs of your enterprise, regardless of size.</p>

<p><strong>Open Source Puppet</strong></p>

<p>Open Source Puppet is the core product found within Puppet Enterprise. It is the engine which drives the Puppet product and does the job of configuration modeling against your environment. It has several components, individually installed, and no shipped console. It generally leads the Enterprise version by several revisions, allowing early access to new features and benefits but lacks the additional features afforded by the Enterprise offering.</p>

<p>Make no mistake. Both products are the same software. However, the Enterprise product has much of the legwork done for you in the integrated installer, additional functionality in independently released “Puppet Apps”, and of course, enterprise-level 24x7 support availability. Add to that a vibrant community of development and expansion, the Puppet Forge, an annual conference, and a respected certification program, and Puppet Labs' offerings, while similar, certainly shine on the Enterprise side of things.</p>

<p><strong>Idempotency</strong></p>

<p>The main concept upon which Puppet’s operation is founded is that of idempotency. Idempotence is the property of certain operations in mathematics and computer science that can be applied multiple times without changing the result beyond the initial application.</p>

<p><img src="http://cvquesty.github.io/images/idempotency.png" alt="Idempotency" /></p>

<p>As you can see, this characteristic only applies an effect on the target if and only if it has not already been applied. Subsequent applications will have no effect, as the desired state of the target is as it should be.</p>

<p>This is good news in that it allows us to think of our enviromnet in new ways. Instead of thinking of all the changes we’re making or going to make to our environment, we begin to considered the desired state of our site and the maintenance of that target at all times. Then, “events” are no longer misconfigurations, but instances of deviation from the desired state, the “norm”.</p>

<p><strong>Implications</strong></p>

<p>Think of the ramifications of the shift in mindset this represents. Audit reports are no longer a series of weeks of review of the site. Your site’s state is described in code, and is a public record within your organization for all to see. Instead, those conversations become “We are ALWAYS compliant. Here is a report of the few times we weren’t compliant in the last year.”</p>

<p>Considerably different conversation to have.</p>

<p>Now, as you model more configuration state in code, your entire way you think of your site evolves. More things hapen automatically, are automatically reported on, and are automatically remediated. Your reporting changes. Your audits change. Your compliance reporting changes. In fact, your entire internal culture chages, and all involved teams need to make adjustments in the way they think about the site and how to mold the way they’ve traditionally done their job (whether administration, audit, or compliance) into this new world of DEVOPS, automation, and the like.</p>

<p><strong>Conclusion</strong></p>

<p>Puppet Enterprise and Open Source promise to not only change how you view your systems and site, but how your entire organization functions. From automated configuration expression within the site, to how you originally model your configuration target, the tools provided by Puppet have changed the face of IT. Automation and configuration management bring culture change and ideological evolution to the enterprise, and step you into the next level of efficiency, compliance, and security management.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Management May Be Missing an Important Component of DEVOPS]]></title>
    <link href="http://questy.org/blog/2015/03/23/management-may-be-missing-an-important-component-of-devops/"/>
    <updated>2015-03-23T10:01:01-04:00</updated>
    <id>http://questy.org/blog/2015/03/23/management-may-be-missing-an-important-component-of-devops</id>
    <content type="html"><![CDATA[<p>As I travel around the country installing and training people in Puppet Enterprise, I’m noticing some characteristics of management perspective on the DEVOPS movement that has a disjoint with implementation and reality. In short: Managers are now hiring personnel in the field of DEVOPS for positions they may be highly qualified for, but have no institutional ability to execute on the tasks that will be assigned them in a modern infrastructure especially one that has to meet governance criteria such as ITIL, PCI, SOX, HIPAA, and various STIG requirements we see in governmental circles.</p>

<p>First a story, then an observation…</p>

<p>Early Rumblings of DEVOPS</p>

<p>A number of years ago I was a senior engineer in a large TV/Web property. The team was probably one of the best I’d ever worked with from an operational perspective. What I mean by that, is they not only knew how to do what they knew, but when confronted with requirements on something that did not exist, or had not been built yet, they just built it themselves. (handy to have in the days before ubiquitous workflow engines, automation tools, and deployment mechanisms!)</p>

<p>At the same time, the devlopment team was mostly tiered… Entry level personnel were basic coders, seniors were considerably more integrated into the nuts &amp; bolts of the site and the leads &amp; managers could actually commit. Quite a well organized protectionist strategy to keep the codebase clean and mostly devoid of errors. It was a great setup for a 2000-2002 era development shop. Problem was, it was 2005.</p>

<p>As business goes, eventually newer and more well rounded developers with experience in a new subset of tools and techniques began to be hired, and from their background they might have had elevated privileges in their past environments, the ability to commit at will (or continuously integrate?) and felt as though this somewhat “experienced” development model was archaic and slow. And it was.</p>

<p>Inevitably, one of these nice folks would make their way over to the operations side of the house, usually in despair, looking for ways to make their lives easier, which usually ended up in some sort of altercation over “root” level access to systems throughout the environment they had to touch. One could assume how that conversation would go, ultimately operations could not find a business justification for such a level of access, and the request was denied. This would engender a certain amount of tension between teams, and life would roll on in much the same way.</p>

<p>Finally, one day, one of the best developers I personally have ever had the privilege of working with came on as a contractor. (he would ultimately come on board full time and then become the Sr. Architect for the team) Everything he did would turn to gold. his development models and abilities were changing the way developers would think about what they could do, and methods and procedures were changing, deployment techologies were being tried, workflow engines were making the development side of the house quite modern by all measures.</p>

<p>However, the existing operational model continued along at the early-century norms, and would not/could not budge. Now, this wasn’t due to the fact that there were jerks in the department, no quite the contrary. In scearios where a team is so competent in what they do, they look for ways to script and automate away mundanity. The better the team, the stronger this backbone. The stronger the backbone, the more tendrils get attached the the core until automation and development reaches each and every part of the infrastructure. When the team builds to that level, each part hands off to the other. Centralized data stores provide the API for the site, and to touch any particular part of the infrastructure at the design level affects the entire system. So goes Infrastructure architecture.</p>

<p>Before there was a “DEVOPS”</p>

<p>As you can see, in very real terms, this was a “pre-incarnate” DEVOPS infrastructure. A little more OPS than DEV, but nonetheless automated as was possible.</p>

<p>But these new upstart tools were going to ruin this! Yes, they had promise and could certainly replace large portions of the existing workflow, but it could take months or years to “undo” what had already been done to supplant existing mechanisms with newer, better tools.</p>

<p>And therein lies the road to DEVOPS.</p>

<p>The Climate Today</p>

<p>I tell the above story to illustrate the tensions existent before the rise of DEVOPS and the subsequent automation revolution we’re currently experiencing.</p>

<p>Many times one would love to implement their new tools, but the operational infrastructure would prevent it. Or, lesser-informed development teams would accept no less than the highes level of access into the environment, but modern compliance standards prevent that from happening as well.</p>

<p>The manager that has to navigate this particular problem when hiring or resourcing a need in her infrastructure has quite the task ahead of them. Why? DEVOPS has integrated the two fields at a vector point to a degree whereby it is incredibly difficult to determine where the DEV ends and the OPS begins. Sure, there are considerably more well-defined responsibilities on the extremeties of the respective disciplines, but that joining point threatens to cause dischord in the world of the IT infrastructure and many sleepless nights for the IT manager in trying to nail down his talent needs.</p>

<p>Take the requirements for the “DEVOPS Application Operations Engineer” found on one of the major online employment sites posted just a few days ago for a major metro in the U.S.:</p>

<p>Minimum 4 years' experience in scripting and or any development languages like C#,.NET, Python, Java, Shell, Ruby or any other open source languages.</p>

<p>Experience with HTML/XML and Java Script</p>

<p>Familiarity with Microsoft SCOM, SolarWinds Orion, Keynote, Nagios, Puppet, Chef or other monitoring, SaaS management solutions is desired</p>

<p>Proven experience debugging and troubleshooting software-related issues in a software development or advanced application support position</p>

<p>As you can see, this is a development-heavy position (that, IMO, is all over the map from a requirements perspective), but so goes job descriptions today. Read between the lines, though…</p>

<p>Someone needs a competent developer that isn’t completely freaked out when someone says “Puppet Environment”, “Monitoring”, or “SaaS”, that knows their way around deployment and automation and can get things done. That’s fine. Problem is, this assumes full lifecycle respnsibility when the actuality is that the future employee has a hard-line stopping point beyond which he or she will never be “allowed” to tread due to compliance alone, and that is the breakpoint between DEV and OPS in the DEVOPS world. Consider this:</p>

<p><img src="http://cvquesty.github.io/images/devops.png" alt="DEVOPS" /></p>

<p>There are three clearly defined worlds here, all converging on a singluar point known as DEVOPS. From the development skill and expertise of the developer to the testing and assurance retrospect of the QA Engineer, to the Security and Compliance purview of the Operational Architect, DEVOPS is not a “one trick pony” with a singularity view. It is a methodology that brings together the three worlds in a clear developmental workflow to speed safe and secure deployment with minimal errors into serving infrastructure. As often as people try and push DEVOPS into a development position or into an Operations or QA postion when hiring, success will be limited, and frustration will be the result.</p>

<p>What, Then, Is the Manager Missing?</p>

<p>As has been heavily implied thus far, the manager may be missing the fact that DEVOPS is not a position but a way of doing things. DEVOPS is a methodology, not a granting of rights or abilities. And, if we’re talking about lines of demarcation within groups, DEVOPS is a superstructure of tools built, implemented and designed by the Operational Architectural team to move, implement, and regression test code and associated objects provided by the Development Architectural team with tests, regressions, and automated mechanisms specified by the Quality Assurance Architectural team in a specific fashion and after a specific methodology that has commonly become referred to as DEVOPS.</p>

<p>The manager has to realize that this is not a subset of bullet points on a resume, but a wholistic approach to all of their environmental considerations that requires all teams to cooperate through to the end result: QUality Software Products delivered in as short a cycle as possible in an automated fashion with as few errors and bugs as reasonably can be remediated before going “live”.</p>

<p>The manager who is looking for this methodology in a single position has already lost the battle before ever posting the position.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vagrant and Docker Love]]></title>
    <link href="http://questy.org/blog/2015/01/15/vagrant-and-docker-love/"/>
    <updated>2015-01-15T20:31:15-05:00</updated>
    <id>http://questy.org/blog/2015/01/15/vagrant-and-docker-love</id>
    <content type="html"><![CDATA[<p>Not a full-on post, but more a note for myself to both investigate and test this… It would seem that Vagrant has now added provisioner support for Docker here. Be looking for a new post on this in the near future!</p>

<p>I’m working steadily to start off the new year, so my posting may be somewhat sporadic, but I will continue to blog Puppet fundamentals, supporting tools, and related items as I have the chance.</p>

<p>And in that vein, I encountered something this week I wanted to share with you.</p>

<p>It would seem that the new feature whereby you can include facter facts in a module for pluginsync to distribute them, but using the new mechanism of:</p>

<p>/modulename/facts.d/external_fact</p>

<p>is not 100% reliable when distributing those facts. By this, I mean the following observed behavior.</p>

<p>You create your fact and place it in the above directory. Say, a shell script that gives you a value.
You make your fact executable, and running it natively at the shell works perfectly.
You do a puppet agent run, and the fact syncs to the agent machine, but never becomes available in the facter table.
You find the sync'ed location of the fact (in the losgs from the sync) and run it manually, and it works perfectly.
I spoke with some folks at Puppet just in a conversation describing what I was seeing and they suggested the following workaround:</p>

<p>Make the fact a file resource and place the fact in /etc/puppetlabs/facter/facts.d. This makes the fact available to the facter system, and displays correctly in the facter table and responds to the facter -p as expected.</p>

<p>That’s it for now! Look for more to come on workflows and tools in the very near future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Moving My Tech Content to GitHub]]></title>
    <link href="http://questy.org/blog/2015/01/08/moving-my-tech-content-to-github/"/>
    <updated>2015-01-08T13:39:12-05:00</updated>
    <id>http://questy.org/blog/2015/01/08/moving-my-tech-content-to-github</id>
    <content type="html"><![CDATA[<p>Happy New Year, all!</p>

<p>Well, you may notice a bit of a change in format &amp; layout. I got tired of fighting the foibles of WordPress. Every few weeks, WordPress decided quite on its own it no longer wished to display my blog for what appeared to be no reason at all.</p>

<p>As such, I’ve moved to Octopress, hosted it at GitHub, and am doing a permanent redirect at it from my site until I can work out both Web &amp; Mail while having my MX stay where it is and having my Web address move to GitHub directly.</p>

<p>In the meantime, all tech posts have been duplicated here, and can be found by navigating the menus.</p>

<p><strong>Note:</strong></p>

<p>Over the next few weeks and months, you’ll see things move around, features being added and removed, themes and plugins changing and/or disappearing as I learn Octopress and figure out all its ins and outs. Please bear with my dist during this time.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PuppetConf 2014]]></title>
    <link href="http://questy.org/blog/2014/09/23/puppetconf-2014/"/>
    <updated>2014-09-23T14:19:37-04:00</updated>
    <id>http://questy.org/blog/2014/09/23/puppetconf-2014</id>
    <content type="html"><![CDATA[<p>Glad to be at PuppetConf with #ShadowSoft exploring all the latest and greatest in PuppetLabs.</p>

<p><a href="http://questy.org/images/puppetconf.jpg"><img src="http://questy.org/images/puppetconf.jpg" alt="puppetconf.jpg" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Southeast Puppet User's Group September]]></title>
    <link href="http://questy.org/blog/2014/09/11/southeast-puppet-users-group-september/"/>
    <updated>2014-09-11T19:33:01-04:00</updated>
    <id>http://questy.org/blog/2014/09/11/southeast-puppet-users-group-september</id>
    <content type="html"><![CDATA[<p><a href="http://questy.org/images/puppet_docker.jpg"><img src="http://questy.org/images/puppet_docker.jpg" alt="puppet_docker" /></a></p>

<p>John Ray is bringing the Puppet + Docker goodness in his talk tonight: &ldquo;Deploying Docker Containers with Puppet&rdquo;.  Join us each month at the Shadow Soft offices for the latest in DEVOPS topics and information.  Always fun, lots of discussion and information surrounding Puppet topics and associated technologies.  There&rsquo;s always pizza and beverages of all kinds, and we&rsquo;ve finally moved into our new meeting/class rooms, so come on out.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Toolbox Grows...]]></title>
    <link href="http://questy.org/blog/2014/06/11/toolbox-grows/"/>
    <updated>2014-06-11T12:57:18-04:00</updated>
    <id>http://questy.org/blog/2014/06/11/toolbox-grows</id>
    <content type="html"><![CDATA[<p>So far we&rsquo;ve gotten our heads around some important things.  First and foremost, vim.  Our editor and companion for creating great code and ways to see our code in action and be able to determine at a glance whether our syntax is correct.  Also, we&rsquo;ve looked at revision control.  The single largest &ldquo;CYA&rdquo; ohmygodimgladivegotanoldercopytorestoreto sort of paradigm where you can roll yourself back to previously &ldquo;known good&rdquo; revisions to save that day&hellip;besides that, it&rsquo;s just darned good practice to keep your code externally saved, revision controlled, and accessible.</p>

<p>I&rsquo;ve also talked about importance of workflow clarity and quality.  If you implement a poor workflow, you just have an automated <em>poor </em>workflow. Key word here is &ldquo;poor&rdquo;.</p>

<p>Next up on our browse through the &ldquo;toolbox&rdquo; is &ldquo;Vagrant&rdquo;.  What is this Vagrant, you ask?</p>

<p>Virtualization is paramount in today&rsquo;s world in a number of ways and for a number of reasons.  For extending your server farms to handle even more application expression, to expand your own desktop machines to test/try different operating systems, and even just rolling up an ad-hoc VM so you can try something without touching a &ldquo;real&rdquo; machine in your environment.</p>

<p>Some may disagree, but I&rsquo;ve found virtualization to be one of the most powerful tools added to the toolbox in years.  Not only can you prototype systems or applications, but you can prototype entire environments.  This is where Vagrant shines, and especially in the context of Puppet (master + clients), allows you to create a fully functioning Puppet environment upon which to develop, prototype, and test without ever jeopardizing even the least important system of your infrastructure.  I count that as a &ldquo;win&rdquo;.  Let&rsquo;s see what this tool can do.</p>

<p><em><strong>What <em>is</em> Vagrant?</strong></em></p>

<p>According to its website:</p>

<p>Vagrant provides easy to configure, reproducible, and portable work environments built on top of industry-standard technology and controlled by a single consistent workflow to help maximize the productivity and flexibility of you and your team.</p>

<p>To achieve its magic, Vagrant stands on the shoulders of giants. Machines are provisioned on top of VirtualBox, VMware, AWS, or any other provider. Then, industry-standard provisioning tools such as shell scripts, Chef, or Puppet, can be used to automatically install and configure software on the machine.</p>

<p>There&rsquo;s a lot there, but it&rsquo;s just a fancy way of saying exactly what I said before.  Vagrant is essentially a framework system that wraps your virtualization engine to manage <em>environments </em>of VMs.  Here is where Vagrant will hold the power for us.</p>

<p><em><strong>Virtualization</strong></em></p>

<p>If Vagrant is the framework, then Virtualization is the foundation.  Now, I&rsquo;ve chosen to use &ldquo;Virtualbox&rdquo; for my virtualization technology, but VMWare works every bit as well.  I am doing all my testing over Virtualbox, however, so YMMV.  Virtualbox is freely available from oracle, and you can download the appropriate version from Virtualbox at <a href="https://www.virtualbox.org.">https://www.virtualbox.org.</a>  I am running the latest version at 4.3.12 (as of this writing) and it serves the Vagrant system extremely well.</p>

<p><em><strong>Vagrant</strong></em></p>

<p>Next, you&rsquo;ll need to install Vagrant on your system.  You can find all the right packages at <a href="http://www.vagrantup.com.">http://www.vagrantup.com.</a>  I am currently running version 1.6.3 without errors.</p>

<p>[warning]I want to make a disclaimer here since I&rsquo;ve had an issue or two with Vagrant on a platform I don&rsquo;t use-Windows.  I am a Mac &amp; Linux user, and have had no issues using the Vagrant/Virtualbox combo on either of these.  However, literally every time I&rsquo;ve used Vagrant over Windows, it&rsquo;s just been a mess.  I&rsquo;ve known one person (ONE!) who has gotten Vagrant to work over Windows, and it required his getting into the product, editing code, etc.  As such, I wouldn&rsquo;t recommend it for those new to the platform.[/warning]</p>

<p>On the Mac platform, you get a .dmg file and can extract it run the installer.  Linux versions are available as RPM installs and Debian Packages.  Once you&rsquo;re installed, let&rsquo;s mess around a bit with Vagrant to see what we can do.</p>

<p><em><strong>Getting Started</strong></em></p>

<p>Vagrant is a unique tool in that it allows you to manage all these varied VMs, but adds a twist.  The big twist is that you don&rsquo;t have to have the <em>source </em>materials for the VMs you&rsquo;re installing.  In fact, the simplicity of turning up a new VM is astounding.  Take the following series of commands:</p>

<p>cd <your favorite directory><br>
mkdir precise32<br>
cd precise32<br>
vagrant init hashicorp/precise32<br>
vagrant up<br></p>

<p>If your Vagrant is installed correctly, a number of things start to happen.  First, Vagrant places a file in your cwd called &ldquo;Vagrantfile&rdquo;.  Your vagrant file (indie) looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># -*- mode: ruby -*-
</span><span class='line'># vi: set ft=ruby :
</span><span class='line'>
</span><span class='line'># All Vagrant configuration is done below. The "2" in Vagrant.configure
</span><span class='line'># configures the configuration version (we support older styles for
</span><span class='line'># backwards compatibility). Please don't change it unless you know what
</span><span class='line'># you're doing.
</span><span class='line'>Vagrant.configure("2") do |config|
</span><span class='line'>  # The most common configuration options are documented and commented below.
</span><span class='line'>  # For a complete reference, please see the online documentation at
</span><span class='line'>  # https://docs.vagrantup.com.
</span><span class='line'>
</span><span class='line'>  # Every Vagrant development environment requires a box. You can search for
</span><span class='line'>  # boxes at https://atlas.hashicorp.com/search.
</span><span class='line'>  config.vm.box = "hashicorp/precise32"
</span><span class='line'>
</span><span class='line'>  # Disable automatic box update checking. If you disable this, then
</span><span class='line'>  # boxes will only be checked for updates when the user runs
</span><span class='line'>  # `vagrant box outdated`. This is not recommended.
</span><span class='line'>  # config.vm.box_check_update = false
</span><span class='line'>
</span><span class='line'>  # Create a forwarded port mapping which allows access to a specific port
</span><span class='line'>  # within the machine from a port on the host machine. In the example below,
</span><span class='line'>  # accessing "localhost:8080" will access port 80 on the guest machine.
</span><span class='line'>  # config.vm.network "forwarded_port", guest: 80, host: 8080
</span><span class='line'>
</span><span class='line'>  # Create a private network, which allows host-only access to the machine
</span><span class='line'>  # using a specific IP.
</span><span class='line'>  # config.vm.network "private_network", ip: "192.168.33.10"
</span><span class='line'>
</span><span class='line'>  # Create a public network, which generally matched to bridged network.
</span><span class='line'>  # Bridged networks make the machine appear as another physical device on
</span><span class='line'>  # your network.
</span><span class='line'>  # config.vm.network "public_network"
</span><span class='line'>
</span><span class='line'>  # Share an additional folder to the guest VM. The first argument is
</span><span class='line'>  # the path on the host to the actual folder. The second argument is
</span><span class='line'>  # the path on the guest to mount the folder. And the optional third
</span><span class='line'>  # argument is a set of non-required options.
</span><span class='line'>  # config.vm.synced_folder "../data", "/vagrant_data"
</span><span class='line'>
</span><span class='line'>  # Provider-specific configuration so you can fine-tune various
</span><span class='line'>  # backing providers for Vagrant. These expose provider-specific options.
</span><span class='line'>  # Example for VirtualBox:
</span><span class='line'>  #
</span><span class='line'>  # config.vm.provider "virtualbox" do |vb|
</span><span class='line'>  #   # Display the VirtualBox GUI when booting the machine
</span><span class='line'>  #   vb.gui = true
</span><span class='line'>  #
</span><span class='line'>  #   # Customize the amount of memory on the VM:
</span><span class='line'>  #   vb.memory = "1024"
</span><span class='line'>  # end
</span><span class='line'>  #
</span><span class='line'>  # View the documentation for the provider you are using for more
</span><span class='line'>  # information on available options.
</span><span class='line'>
</span><span class='line'>  # Define a Vagrant Push strategy for pushing to Atlas. Other push strategies
</span><span class='line'>  # such as FTP and Heroku are also available. See the documentation at
</span><span class='line'>  # https://docs.vagrantup.com/v2/push/atlas.html for more information.
</span><span class='line'>  # config.push.define "atlas" do |push|
</span><span class='line'>  #   push.app = "YOUR_ATLAS_USERNAME/YOUR_APPLICATION_NAME"
</span><span class='line'>  # end
</span><span class='line'>
</span><span class='line'>  # Enable provisioning with a shell script. Additional provisioners such as
</span><span class='line'>  # Puppet, Chef, Ansible, Salt, and Docker are also available. Please see the
</span><span class='line'>  # documentation for more information about their specific syntax and use.
</span><span class='line'>  # config.vm.provision "shell", inline: &lt;&lt;-SHELL
</span><span class='line'>  #   apt-get update
</span><span class='line'>  #   apt-get install -y apache2
</span><span class='line'>  # SHELL
</span><span class='line'>end</span></code></pre></td></tr></table></div></figure>


<p>Note that this is a long file with a lot of explanatory documentation.  In actuality, the most important part of your Vagrantfile can be summed up here:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># -*- mode: ruby -*-
</span><span class='line'># vi: set ft=ruby :
</span><span class='line'>Vagrant.configure("2") do |config|
</span><span class='line'>  config.vm.box = "hashicorp/precise32"
</span><span class='line'>end</span></code></pre></td></tr></table></div></figure>


<p>These are the lines that are uncommented plus the top two declaratives that tell Vagrant what to do.  It&rsquo;s a very simple file that does some very powerful things.  First, it checks your home directory in the ~/.vagrant.d location to see if you already have the &ldquo;precise32&rdquo; Vagrant source &ldquo;box&rdquo;.  (more on boxes later).  Next, if you do have this, it will simply start up a VM in your virtualization of choice with a randomized name.  For instance, mine is called &ldquo;precise32_default_1402504453444_30545&rdquo;.  Vagrant takes away the selection of an .iso image, connecting it to the virtual CD/DVD Rom, starting an installer, etc.  It simply sends you a pre-rolled image, places it in your .vagrant.d directory, and provisions the VM to respond to Vagrant commands, and starts it up within Virtualbox.  Precise32 is simply a test scenario, as Vagrant&rsquo;s site has quite a number of varied and specially configured &ldquo;box&rdquo; files that you can use to prototype on at their &ldquo;ready-made&rdquo; box discovery site: <a href="https://vagrantcloud.com/discover/featured.">https://vagrantcloud.com/discover/featured.</a>  You can install boxes with too many variations and differentiations to enumerate here, and that&rsquo;s not really the point for our purposes&hellip; you may find these of great assistance in your own workplace, but let&rsquo;s continue.</p>

<p>When you run your &ldquo;vagrant init&rdquo; command listed above, it places a Vagrantfile, and when you do a &ldquo;vagrant up&rdquo;, it automatically retrieves your box file, provisions, and starts the VM.  Now, by simply running &ldquo;vagrant ssh default&rdquo;, you are now logged into this virtual machine!  You also have full sudo to become root and do any sort of damage you may wish to do.  If you logout (&ldquo;exit&rdquo; or CTRL-D), and type &ldquo;vagrant destroy&rdquo;, the VM goes away and you have nothing in Virtualbox.</p>

<p>Were we to just stop here, the power inherent in being able to just have these &ldquo;Vagrantfiles&rdquo; (sort of like a &ldquo;Makefile&rdquo; for boxes) to spin up and down test scenarios at will is incredible.  But, let&rsquo;s look at this in light of the Vagrantfile, what it can do and how you can customize it.  There is an entire descriptive language surrounding Vagrant PLUS Vagrant has a plugin infrastructure whereby developers can extend Vagrant&rsquo;s capabilities.  We will capitalize on these later.</p>

<p>So, imagine a scenario where you can create a directory, copy a text file into it, run a single command, and it automatically provisions a 4-node Puppet Enterprise infrastructure, fully installed with a master and three agents, MCollective fully installed, PuppetDB installed and in use&hellip;  literally a full installation just like you would use for your infrastructure&hellip;  Now we get powerful.  NOW we have the ability to do some cool things.</p>

<p>Next time, that&rsquo;s exactly what we&rsquo;re going to do.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA["Do's and Dont's" for Your Puppet Environment]]></title>
    <link href="http://questy.org/blog/2014/05/20/dos-donts-puppet-environment/"/>
    <updated>2014-05-20T09:15:14-04:00</updated>
    <id>http://questy.org/blog/2014/05/20/dos-donts-puppet-environment</id>
    <content type="html"><![CDATA[<p>IT Automation, like the features and functions offered by Puppet, is riddled with a number of pitfalls. Nothing dangerous or site-threatening in the near term, however evolving a <em>bad </em>plan can lead you down a painful path to re-trek when you ultimately need to demolish what you&rsquo;ve done and re-tool, re-work, or even re-start from scratch.  Some simple suggestions can help smooth your integration, and also provide tools and methodologies that make changes in philosophy easy to test and implement as well as make the long road back from a disaster easy(-ier?) to navigate.</p>

<p>Here are some simple guidelines that can provide that foundation and framework:</p>

<p><strong>DO <em>Always</em> Use Revision Control</strong></p>

<p>It would seem this would be a foregone conclusion in this day and age, but you would be surprised just how many shops don&rsquo;t have revision control of any kind in place.  A series of manifests or configurations might be tarred up and sent to the backup system, but aside from dated backups, there&rsquo;s no real versioning&hellip;just monolithic archives to weed through in a time of disaster.</p>

<p>Revision control puts you one command away from restoring those configurations and manifests (and even your data vis-a-vis &ldquo;Hiera&rdquo;) to their original locations in the most recent state.</p>

<p><strong>DO Rethink Your Environments</strong></p>

<p>If you automate a bad workflow, you still have a bad workflow.  (albeit an automated one!)</p>

<p>Rethink how you do things and why.  Why do you promote code the way you do, and is there a better way to do it?  Why do you still have a manual portion to your procedure, and is it entirely necessary, or can this be remanded to Puppet to do for you as well.  What things are you doing well?  How can they improve?</p>

<p>Try to think through all your procedures.  There are more than you think, and they&rsquo;re often less optimized than they can be.  If you&rsquo;re going to implement Puppet automation, it&rsquo;s time to retool.</p>

<p><strong>DO Implement Slowly and Methodically</strong></p>

<p>Another pitfall a lot of shops wander into is they try to do too much all at once, and do none of it well.  Either they implement too quickly and migrate a huge environment it took years to build (sometimes as much as a decade!) through a single professional services engagement or at an unrealistic pace.  Automation is complex, but if you take the time to implement correctly, piece-by-piece and hand-in-hand with your rethinking of your environment referred to above, you can revolutionize the way you work and make the environment considerably more powerful, considerably easier to work with, and ultimately release yourself to work on much more interesting problems in your environment.  Take your time to build the environment you want.</p>

<p><strong>DO Engage the Community</strong></p>

<p>By using Puppet, you are the beneficiary of the greatest software development paradigm in history &ndash; the Open Source movement.  People all over the world have taken part in crafting the powerful tool you have before you.  If you are able to help in like manner, by all means contribute your code to the community. (With your data in Hiera, this is easier than ever!)  Join a Puppet Users Group.  Share your clever solutions to unique problems with the community via GitHub, the Puppet Forge, your website&hellip; give back.  The more you pour in, the more you get out, and something you solve may end up baked into the final product one day in the future.</p>

<p><strong>DON&rsquo;T Pit Teams Against Each Other</strong></p>

<p>DON&rsquo;T make this a DEV vs OPS paradigm.  This is a marriage of the best tools of both worlds.  Depending on how your culture breaks down, this could be an OPS-aware way of doing development, or a DEV-informed way of doing operations.  You need to remember one thing in all of it.  The marriage of these worlds is a <em>teamwork</em> effort.</p>

<p>I was averse to the term DEVOPS when it first started being used, as it was a tool of the development world I was engaged with to cede root level access to developers.  In a properly managed, secure environment, this is always a no-no.  Development personnel are not trained systems people and rarely are.  By the same token, never ask your systems people to delve into core development, or to troubleshoot your developers' code.  They are not tooled for that work.</p>

<p>This does not say that one is better than the other, nor does it say they do not share a certain amount of core skills at the basest levels. Much like the differences between civil and mechanical engineers, each has a base level of knowledge that ties them together, but each is highly specialized.  You don&rsquo;t want your civil engineer building machine tools just as much as you don&rsquo;t want your mechanical engineer building bridges.  Each discipline is highly specialized and carries with it nuance and knowledge you only gain through experience&hellip;experience <em>on </em>the job.</p>

<p>Instead, find a culture and a paradigm that joins the forces of these two disciplines to build something unique and special rather than wasting time with dissension and argument.</p>

<p><strong>DON&rsquo;T Expect Automation to Solve <em>Everything</em></strong></p>

<p>I know, that sounds like a sacrilege at this point, but its true.  No matter how automated your site becomes, how detailed your configuration elements are, or how much you&rsquo;ve detailed your entire workflow, you still can never replace the element of human consideration and decision-making.</p>

<p>Automation, as I&rsquo;ve said before, automates away the mundane to make time for you, DEVOPS person, to work on really interesting and curious work.  You can now write that entire new whiz bang gadget you&rsquo;ve been conceptualizing for the last several years, but have never quite gotten there because you were too busy &ldquo;putting out fires&rdquo;.  Puppet automation is definitely a watershed in modern administration and development, but people are still needed.</p>

<p>Another &ldquo;intangible&rdquo; you may not readily think about when considering a DEVOPS infrastructure is one of <em>culture.  </em>The best places to work are always the best cultures brought about by the right collection of people, ideas, personalities, and management styles.  When you find that right mix of people and ideas, the workplace becomes a, forgive me, <em>magical</em> place to be.  Automation can never make that happen.</p>

<p><strong>DON&rsquo;T Starve Your Automation Environment</strong></p>

<p>Automation solves a lot of things, but one thing it cannot do is feed itself.  This particular animal has a ton of needs over time.  From appropriate hardware to personnel, the environment needs time, attention, and consideration.  Remember that this is the &ldquo;machine tool&rdquo; of your whole company.  It is the thing that builds and maintains other things.  As such, its priority rises above that of the next web server or DNS system.</p>

<p>Always allocate enough resources (read: money, personnel, and time) to your environment.  If that means engineer time to work on a specially project and to do the job <em>right</em>, that&rsquo;s what it means.  And, yes, it&rsquo;s more important than meeting an arbitrarily assigned &ldquo;live date&rdquo; to your new widget or site or application.  The environment comes first, and all else follows.  If you give the resources and time to your automation initiatives it deserves, a number of years down the road you will look back and be amazed at the sheer amount of work your team was able to accomplish just by keeping this simple precept.</p>

<p><strong>DON&rsquo;T Stop Evolving</strong></p>

<p>Never stop learning.  Never stop bettering yourself or your environment.  Always keep refactoring your code.  (i.e., if you wrote that Apache module 4 years ago, chances are good that what you&rsquo;ve learned in the interim can go back into making that module even better.)  Always keep your people trained and engaged on the latest developments in Puppet and all the associated tools.  Never stop striving to be better and never stop reaching.  I may sound lil your coach from high school in this, but those principles he was trying to impart hold true.  If you continue to drive forward and reinvent yourself as a regular part of your forward pursuits, the endpoint of that evolution will benefit you personally, your team both vocationally and culturally, your company&rsquo;s efficiency, and your environment&rsquo;s impact on your bottom line.</p>

<p><strong>Conclusion</strong></p>

<p>If we keep a stronger eye on our environment and tools that rises above the simple concept of &ldquo;that software I bought&rdquo; and &ldquo;fit it in between all the other things you have to do&rdquo; and give Puppet its proper place in our company, it can truly revolutionize your workflow.  However, when properly placed culturally and from a design, implementation, and workflow perspective, it can transform any shop on levels not readily observable when looking at the price tag or the resource requirements list.  DO let Puppet transform your environment and workflow and DON&rsquo;T be afraid to take the plunge.  It&rsquo;s exciting, challenging, and can easily take your company to the &ldquo;next level&rdquo;.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GitHub, Git, and Just Plain Revision Control]]></title>
    <link href="http://questy.org/blog/2014/05/13/github-git-just-plain-revision-control/"/>
    <updated>2014-05-13T17:06:09-04:00</updated>
    <id>http://questy.org/blog/2014/05/13/github-git-just-plain-revision-control</id>
    <content type="html"><![CDATA[<p>One of the &ldquo;bugaboos&rdquo; in the sysadmin world for the longest time was the reluctance to use those &ldquo;stinky developer tools&rdquo; in our world for any reason.  I&rsquo;m not sure the impetus behind this, but my wager is on something akin to security or yet another open port or &ldquo;attack vector&rdquo; if you will.  But today&rsquo;s competent and conscientious systems admin (not to mention DEVOPS person) will use revision control as their go-to standard for collecting, versioning, backing up, and distributing all manner of things.</p>

<p>I&rsquo;ve seen some shops use <a href="http://www.nongnu.org/cvs/">CVS</a> as their choice, old thought it is, just as a large &ldquo;bucket&rdquo; in which to throw things for safekeeping with revisions and rollbacks available in case of some uncertain as yet unencountered event.  <a href="http://subversion.apache.org">Subversion</a> was the next generation of revision control tools.  Darling of developers and bane of disk space, Subversion still had many more features and performed essentially the same task.</p>

<p>Now, <a href="http://git-scm.com">Git</a> is the flavor of the month, and not only has gained widespread acceptance as a standard way to &ldquo;do&rdquo; revision control, it&rsquo;s the de-facto way to do DEVOPS in  a Puppet world.  Granted, there are those brave souls out there who have tried to stick with the older tools, but the workflow and the &ldquo;glue&rdquo; between all the various components therein.  Hence, this post.</p>

<p><strong>What is Git, really?</strong></p>

<p>Git was developed by <a href="https://en.wikipedia.org/wiki/Linus_Torvalds">Linus Torvalds</a> for Linux Kernel collaboration.  He needed a new revision control system akin to the previously used <a href="https://en.wikipedia.org/wiki/BitKeeper">BitKeeper</a> software that was unencumbered by copyright and able to handle the unique distributed development needs of the Linux project.  So, rather than try and use someone else&rsquo;s project, he collated what was needed and developed the project himself.</p>

<p>Now, Git is used both privately and Publicly throughout the world for many projects.  Git is lightweight and works in a more efficient manner by moving changes via diffs rather than whole repositories, allows developers to maintain and manage an entire repository on their own systems either connected or disconnected from the Internet.  Then, they can &ldquo;push&rdquo; all their changes back to the central repository as needed.</p>

<p><strong>Enter GitHub</strong></p>

<p>For our purposes, we&rsquo;ll specifically be working in <a href="https://en.wikipedia.org/wiki/GitHub">GitHub</a>.  GitHub is a project offering web-based hosting of your code that you can source from anywhere.  GitHub offers public and private hosting and a spate of other related services to development collaboration on the Internet.  If you do not have a GitHub account, you&rsquo;ll need to surf on over to the site and sign up for one.  It&rsquo;s free and it&rsquo;s fast, and I&rsquo;ll be using and sourcing it heavily as this series continues.</p>

<p><strong>Basic Git</strong></p>

<p>Git itself is available on most modern platforms and can easily hook into GitHub for our purposes.  I will be mostly referring to command-line usage of git, but you will find quite a bit in the way of tools, frontends, and &ldquo;helper&rdquo; apps for Git that you may or may not wish to leverage as you learn and incorporate Git into your workflow.  In the meantime, stick with me on command-line work.</p>

<p>When you install git on your unix-like platform, it will drop a few binaries.  The one we&rsquo;re most interested in is the git binary itself.  It&rsquo;s very simply designed and has a very straightforward set of options you can get from the command line by simply typing &ldquo;git&rdquo; with no options, or &ldquo;git help&rdquo;.  The output is below:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>usage: git [--version] [--help] [-C &lt;path&gt;] [-c name=value]
</span><span class='line'>           [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path]
</span><span class='line'>           [-p | --paginate | --no-pager] [--no-replace-objects] [--bare]
</span><span class='line'>           [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;]
</span><span class='line'>           &lt;command&gt; [&lt;args&gt;]
</span><span class='line'>
</span><span class='line'>These are common Git commands used in various situations:
</span><span class='line'>
</span><span class='line'>start a working area (see also: git help tutorial)
</span><span class='line'>   clone      Clone a repository into a new directory
</span><span class='line'>   init       Create an empty Git repository or reinitialize an existing one
</span><span class='line'>
</span><span class='line'>work on the current change (see also: git help everyday)
</span><span class='line'>   add        Add file contents to the index
</span><span class='line'>   mv         Move or rename a file, a directory, or a symlink
</span><span class='line'>   reset      Reset current HEAD to the specified state
</span><span class='line'>   rm         Remove files from the working tree and from the index
</span><span class='line'>
</span><span class='line'>examine the history and state (see also: git help revisions)
</span><span class='line'>   bisect     Use binary search to find the commit that introduced a bug
</span><span class='line'>   grep       Print lines matching a pattern
</span><span class='line'>   log        Show commit logs
</span><span class='line'>   show       Show various types of objects
</span><span class='line'>   status     Show the working tree status
</span><span class='line'>
</span><span class='line'>grow, mark and tweak your common history
</span><span class='line'>   branch     List, create, or delete branches
</span><span class='line'>   checkout   Switch branches or restore working tree files
</span><span class='line'>   commit     Record changes to the repository
</span><span class='line'>   diff       Show changes between commits, commit and working tree, etc
</span><span class='line'>   merge      Join two or more development histories together
</span><span class='line'>   rebase     Reapply commits on top of another base tip
</span><span class='line'>   tag        Create, list, delete or verify a tag object signed with GPG
</span><span class='line'>
</span><span class='line'>collaborate (see also: git help workflows)
</span><span class='line'>   fetch      Download objects and refs from another repository
</span><span class='line'>   pull       Fetch from and integrate with another repository or a local branch
</span><span class='line'>   push       Update remote refs along with associated objects
</span><span class='line'>
</span><span class='line'>'git help -a' and 'git help -g' list available subcommands and some
</span><span class='line'>concept guides. See 'git help &lt;command&gt;' or 'git help &lt;concept&gt;'
</span><span class='line'>to read about a specific subcommand or concept.</span></code></pre></td></tr></table></div></figure>


<p>We&rsquo;re most interested in a small subset of commands for our purposes here.  They are <strong>add, commit, pull, push, branch, checkout, and clone.</strong></p>

<p>I will be referencing one particular way to &ldquo;do&rdquo; git which works for me, but as with anything <a href="https://en.wikipedia.org/wiki/TMTOWTDI">TMTOWTDI</a> and <a href="https://en.wiktionary.org/wiki/YMMV">YMMV</a>.</p>

<p><strong>GitHub Portion</strong></p>

<p>I am going to assume you&rsquo;ve created a GitHub Account.  When you create your account, you&rsquo;ll have a unique URL assigned to you based on your username.  Mine, for instance, is <a href="https://github.com/cvquesty/">https://github.com/cvquesty/</a><insert project name here>.  The basic interface to GitHub is rather straightforward and looks like the following:</p>

<p><a href="http://questy.org/images/mygithub.png"><img src="http://questy.org/images/mygithub.png" alt="mygithub" /></a></p>

<p>The interface keeps track of all projects you&rsquo;re working on, the frequency with which you commit or otherwise use your repository, and (most importantly), a centralized server that is storing those projects you can source from any internet connected system.</p>

<p><strong>Make a Repository</strong></p>

<p>In the upper right-hand corner of your screen, you&rsquo;ll notice a &ldquo;+&rdquo; symbol.  Let&rsquo;s click that and create us a new repository.  You&rsquo;ll be presented with a dialog to name and describe your new repo.  I&rsquo;ll use the name &ldquo;sample repo&rdquo; and the description &ldquo;Sample Repo for my Tutorial&rdquo; with no other options other than the defaults.  (we&rsquo;ll go manually through those processes shortly).  After creating the repository by clicking &ldquo;Create Repository&rdquo;, I&rsquo;m presented with a page that has step-by-step instructions on what to do next.  I&rsquo;ll include that here for you.</p>

<p><a href="http://questy.org/images/myrepo.png"><img src="http://questy.org/images/myrepo.png" alt="myrepo" /></a></p>

<p>As you can see, you have your repo referenced at the top by <userid>/<reponame>.  You have instructions on how to use the repo from both the GitHub desktop client and the command line and some special instructions for if you have it locally on your system, and are just now uploading that content into this repository you&rsquo;ve created to hold it.  We&rsquo;re interested in the command line instructions.</p>

<p><strong>A Place to Git</strong></p>

<p>On my system (a Mac), I have Git installed by default and I have a directory in my home directory simply called &ldquo;Projects&rdquo;.  Under there, I have a &ldquo;Git&rdquo; directory.  ALL of my work in Git goes here.  This is not a hard/fast rule.  I just chose it as my location to place all my git work so it is centralized and all collected together.</p>

<p>What we&rsquo;re going to do next is to configure Git, create a location for our repo, make a file to commit to the repo and then push that file up to GitHub to see how that workflow works.  Let&rsquo;s get started.</p>

<p><strong>Configuring Git</strong></p>

<p>Since Git is personal to you as a user, you need to let Git know a few things about you.  This gives your git server (in our case GitHub) the information it needs when you&rsquo;re pushing code (like your identity, default commit locations, etc).  First, your name and email:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git config --global user.name "John Doe"
</span><span class='line'>git config --global user.email you@yourmail.com</span></code></pre></td></tr></table></div></figure>


<p>You&rsquo;ll only need to perform this once.  There are quite a few options and you can read up on those <a href="http://git-scm.com/book/en/Getting-Started-First-Time-Git-Setup">here</a> at your leisure.</p>

<p>Next, create a location for your new repo.  I chose my aforementioned directory and created the location:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/Users/jsheets/Projects/Git/samplerepo</span></code></pre></td></tr></table></div></figure>


<p>for demonstration purposes.  From here, though, we can take up with the instructions on the GitHub page displayed after creating your repo. I&rsquo;ll reproduce that here for reference:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd /Users/jsheets/Projects/Git/samplerepo
</span><span class='line'>touch README.md
</span><span class='line'>git init
</span><span class='line'>git add README.md
</span><span class='line'>git commit -m "first commit"
</span><span class='line'>git remote add origin https://github.com/cvquesty/samplerepo.git
</span><span class='line'>git push -u origin master</span></code></pre></td></tr></table></div></figure>


<p>If all has gone well, you have now created an empty README.md file, committed it to your local Git repository and then subsequently pushed it up to GitHub.  You&rsquo;ll note that we added &ldquo;origin&rdquo; as the remote and then we pushed to &ldquo;origin&rdquo; in a thing called &ldquo;master&rdquo;.  What&rsquo;s that all about?</p>

<p>GitHub (and git) refer to their repo location as &ldquo;origin&rdquo;.  This becomes handy when you start pushing between remote repositories and from remote to remote to GitHub, etc.  So, it makes sense to name GitHub <em>functionally </em>as well as it&rsquo;s assigned domain name.  By saying &ldquo;origin&rdquo;, we&rsquo;re making GitHub the de-facto standard center of everything we&rsquo;re doing.</p>

<p>Next, we refer to &ldquo;master&rdquo;.  What is that?  Simply stated, we&rsquo;re pushing to a &ldquo;branch&rdquo; called &ldquo;master&rdquo;.</p>

<p><strong>Branching</strong></p>

<p>Branching is a method by which you can have multiple code &ldquo;branches&rdquo; or &ldquo;threads&rdquo; in existence simultaneously, and Git is managing them all for you.  For instance, you may wish to have one code collection only for use in production systems while maintaining a separate one for development systems.  In fact, you can create a random branch with a bug name (bug1234, for instance), commit your changes to that, test it, and push it to origin, then pull it down to all your production hosts, solving a big problem in your site or codebase.  Better yet, if it all works great and you&rsquo;re happy with it, you can &ldquo;merge&rdquo; that bug back into your main code repository, making it a permanent fixture in your code in whatever branch you like. (or even all of them!)</p>

<p>When you first create your repo, GitHub makes a &ldquo;main&rdquo; branch for you automatically, and calls it &ldquo;master&rdquo;.  So, by utilizing the command above, we&rsquo;re telling Git to push our code (in this case, README.md) to our origin server (GitHub) and put it in the &ldquo;master&rdquo; branch.</p>

<p>While we&rsquo;re on the topic, let&rsquo;s create two more branches so we can get the full hang of this branching thing.  (Hint:  It&rsquo;s core to how we integrate this into Puppet).</p>

<p><strong>Makin' Branches</strong></p>

<p>As I said before, GitHub creates a default &ldquo;master&rdquo; branch for you.  If, from your local repository location, you type &ldquo;git branch&rdquo;, Git will list a single branch for you,</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git branch
</span><span class='line'>* master</span></code></pre></td></tr></table></div></figure>


<p>This tells us simply, what branch you are currently in.  Now, let&rsquo;s run two commands to create new branches to be tracked by Git.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git branch production
</span><span class='line'>git branch development</span></code></pre></td></tr></table></div></figure>


<p>Now.  Run &ldquo;git branch&rdquo; again:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git branch
</span><span class='line'>development
</span><span class='line'>* master
</span><span class='line'>production</span></code></pre></td></tr></table></div></figure>


<p>As you can see, your other branches are now visible when running the command.  If you have color, you may notice that the &ldquo;master&rdquo; branch is a different color than the others (based on your settings).  If you do not have color, the asterisk denotes what branch is active as well.</p>

<p><strong>Checkout and Commit, Branch and Merge</strong></p>

<p>We have our repository and we have our branches.  We have a single README.md in the current directory, and we are ready to roll committing code and pushing it into our repository.  Let&rsquo;s perform a simple experiment to get the &ldquo;hang&rdquo; of how the branches work and how to switch between them as needed.  Since we&rsquo;re in &ldquo;master&rdquo;, let&rsquo;s edit our README.md to reflect that by placing a single word in the file &ldquo;master&rdquo;. (use vim as discussed in our last tutorial).</p>

<p>Once you&rsquo;re done with your edit, you&rsquo;ll see that the text is in the file.  you can edit it and you can cat the file and see the contents, but if you view the file up at GitHub, that content is not there yet.  Some sort of way, a mechanism must be used to put that data there.  Well, there is such a process, and it is a two part process.</p>

<p>Recall I mentioned that one of the features of Git is that you can have a complete repository local to your machine.  you can work on that repo and make all sorts of changes completely disconnected from your server (in our case GitHub&hellip; &ldquo;origin&rdquo; as it is named to Git).  Therefore, in reality you are dealing with not one, but two repositories.  The local one on your machine and the remote one at origin.  (remember the &ldquo;git remote add origin&rdquo; above?)</p>

<p>So, to finalize your changes locally, you must &ldquo;commit&rdquo; them to your local repository as &ldquo;final&rdquo;.  THEN, you can &ldquo;push&rdquo; those changes into your main server (in our case GitHub).   We did as much above with our procedure where we did the commit with a message, and then a push up to origin.  However, now that we&rsquo;ve made changes locally, they are not yet reflected at GitHub.  Logic would dictate another commit is in order:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git commit
</span><span class='line'>or
</span><span class='line'>git commit -m 'Some message about your commit'</span></code></pre></td></tr></table></div></figure>


<p>As you can see, there are two routes you can go.  If you simply supply the &ldquo;git commit&rdquo; without any options, you will be brought into the system text editor you (or your OS) has configured in the $EDITOR environment variable.  Most platforms use &ldquo;vi&rdquo; or &ldquo;vim&rdquo; for this, but I have also seen &ldquo;pico&rdquo; used in some distributions like Ubuntu Linux.  In any event, you can edit the file by placing your comments in.  After exiting the file, saving the content, the commit will be complete.  If, however, you do not put anything, git will not commit the changes.  This is to enforce good coding practice by requiring some notes about what a committer is doing before making the changes.  It&rsquo;s a highly recommended workflow to follow.</p>

<p>Once your commit is complete, phase 1 (local commit) is over.  You can commit over and over, as many times as you like.  you are a full, local repository.  In fact, I&rsquo;d encourage many commits.  Commit when you think about it.  Commit before you walk away from your system.  Commit randomly for no reason in mid-workflow.  The more commits you have, the less likely you are to lose work.</p>

<p>Finally, to get the data up to GitHub, we need to &ldquo;push&rdquo; that data off your repository and into your &ldquo;origin&rdquo; repository. This is quite simple, and you&rsquo;ve done it before:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git push -u origin master</span></code></pre></td></tr></table></div></figure>


<p>Sometimes you may wish to not keep specifying the location you&rsquo;re pushing to.  If so, you can set a default location for each branch.  Git will tell you just how to do that if you forget the &ldquo;-u location branch&rdquo; option.  Let&rsquo;s say I&rsquo;m in my aster branch and I simply run a &ldquo;git push&rdquo;.  Git will tell me I did something wrong, but will <em>also </em>tell me how to eliminate that problem:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>fatal: The current branch master has no upstream branch.
</span><span class='line'>To push the current branch and set the remote as upstream, use
</span><span class='line'>
</span><span class='line'>git push --set-upstream origin master</span></code></pre></td></tr></table></div></figure>


<p>&ldquo;fatal&rdquo; seems a <em>little </em>melodramatic since Git gives you the answer as to what to do right there.  All you need to do is set the default target once with that last line, and from that point forward, you only need type &ldquo;git push&rdquo; when pushing to GitHub.  Hint:  I do this in ALL my branches at create time.  It saves a lot of typing over time, and like any good Sysadmin, I&rsquo;m lazy.  :)</p>

<p>So, now I&rsquo;ve got multiple branches that need this setting, but I&rsquo;m still stuck in &ldquo;master&rdquo;.  How do I get to &ldquo;development&rdquo; or &ldquo;production&rdquo; to perform the same tasks?</p>

<p>Git provides a &ldquo;checkout&rdquo; command.  What you&rsquo;re saying with &ldquo;checkout&rdquo; is:  "Git, I want to be working on branch &ldquo;x&rdquo;, and I want you to make that my current branch.  if there are any differences between that branch and the one I&rsquo;m on, please make those changes on-disk for me so I can exclusively be working in branch &ldquo;x&rdquo;&ldquo;.  A little verbose, but you get the point.  So, to move to the next branch and do all the wonderful things we did in "master&rdquo; above, we perform:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git checkout development
</span><span class='line'>edit README.md to say different text
</span><span class='line'>git commit -a -m 'editing README for development branch'
</span><span class='line'>git push --set-upstream origin development
</span><span class='line'>git push</span></code></pre></td></tr></table></div></figure>


<p>If all has gone well, your development README.md file is now changed and pushed into GitHub.  What about &ldquo;master&rdquo;, though?  Well, let&rsquo;s take a look:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git checkout master
</span><span class='line'>cat README.md</span></code></pre></td></tr></table></div></figure>


<p>If all has gone well, the contents of README.md are back to what was in your &ldquo;master&rdquo; branch.  By checking out &ldquo;development&rdquo;, it&rsquo;ll change back to the new content there.  As a test, checkout the &ldquo;production&rdquo; branch, change the README.md file, commit it, set your upstream push target and then push the contents to GitHub.</p>

<p>Now you&rsquo;re cooking with gas.</p>

<p><strong>Conclusion</strong></p>

<p>This is a simple tutorial to get you started with Git &amp; GitHub.  There are MANY tutorials and books that can make you into a Git expert, but are way outside the scope of this humble little blog.  let me provide a few of those for you here:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[Git Help](http://git-scm.com/documentation)
</span><span class='line'>[Git Book](http://git-scm.com/book)[
</span><span class='line'>GitHub Help](https://help.github.com)</span></code></pre></td></tr></table></div></figure>


<p>This documentation should be more than enough to get you moving and well underway with Git ins-and-outs for committing Puppet code and using r10k to interface with and distribute that code around your environment.</p>
]]></content>
  </entry>
  
</feed>
